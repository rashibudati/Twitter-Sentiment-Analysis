{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Twitter-Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "eerwLgqRtG3f",
        "colab_type": "code",
        "outputId": "796acfc3-c742-485e-8727-b9a97073f08a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypU70Y6LtVCc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.chdir('/content/gdrive/My Drive/CS291K-master/')\n",
        "#!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QdH6IWKvcGx",
        "colab_type": "text"
      },
      "source": [
        "CNN-LSTM File\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7s1-Qh9t0ik",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        },
        "outputId": "d89f6594-7cba-44d5-e088-18e5bee5d1b1"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from IPython import embed\n",
        "\n",
        "class CNN_LSTM(object):\n",
        "    def __init__(self, sequence_length, num_classes, vocab_size, embedding_size, filter_sizes, num_filters, l2_reg_lambda=0.0,num_hidden=100):\n",
        "\n",
        "        # PLACEHOLDERS\n",
        "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")    # X - The Data\n",
        "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")      # Y - The Lables\n",
        "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")       # Dropout\n",
        "\n",
        "        \n",
        "        l2_loss = tf.constant(0.0) # Keeping track of l2 regularization loss\n",
        "\n",
        "        #1. EMBEDDING LAYER ################################################################\n",
        "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
        "            self.W = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0),name=\"W\")\n",
        "            self.embedded_chars = tf.nn.embedding_lookup(self.W, self.input_x)\n",
        "            self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n",
        "\n",
        "        #2. CONVOLUTION LAYER + MAXPOOLING LAYER (per filter) ###############################\n",
        "        pooled_outputs = []\n",
        "        for i, filter_size in enumerate(filter_sizes):\n",
        "            with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
        "                # CONVOLUTION LAYER\n",
        "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
        "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
        "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
        "                conv = tf.nn.conv2d(self.embedded_chars_expanded, W,strides=[1, 1, 1, 1],padding=\"VALID\",name=\"conv\")\n",
        "                # NON-LINEARITY\n",
        "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
        "                # MAXPOOLING\n",
        "                pooled = tf.nn.max_pool(h, ksize=[1, sequence_length - filter_size + 1, 1, 1], strides=[1, 1, 1, 1], padding='VALID', name=\"pool\")\n",
        "                pooled_outputs.append(pooled)\n",
        "\n",
        "        # COMBINING POOLED FEATURES\n",
        "        num_filters_total = num_filters * len(filter_sizes)\n",
        "        self.h_pool = tf.concat(pooled_outputs, 3)\n",
        "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
        "        \n",
        "        #3. DROPOUT LAYER ###################################################################\n",
        "        with tf.name_scope(\"dropout\"):\n",
        "             self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
        "\n",
        "        #4. LSTM LAYER ######################################################################\n",
        "        cell = tf.contrib.rnn.LSTMCell(num_hidden,state_is_tuple=True)\n",
        "        self.h_drop_exp = tf.expand_dims(self.h_drop,-1)\n",
        "        val,state = tf.nn.dynamic_rnn(cell,self.h_drop_exp,dtype=tf.float32)\n",
        "        \n",
        "        #embed()\n",
        "\n",
        "        val2 = tf.transpose(val, [1, 0, 2])\n",
        "        last = tf.gather(val2, int(val2.get_shape()[0]) - 1) \n",
        "\n",
        "        out_weight = tf.Variable(tf.random_normal([num_hidden, num_classes]))\n",
        "        out_bias = tf.Variable(tf.random_normal([num_classes]))\n",
        "\n",
        "        with tf.name_scope(\"output\"):\n",
        "            #lstm_final_output = val[-1]\n",
        "            #embed()\n",
        "            self.scores = tf.nn.xw_plus_b(last, out_weight,out_bias, name=\"scores\")\n",
        "            self.predictions = tf.nn.softmax(self.scores, name=\"predictions\")\n",
        "\n",
        "        with tf.name_scope(\"loss\"):\n",
        "            self.losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores,labels=self.input_y)\n",
        "            self.loss = tf.reduce_mean(self.losses, name=\"loss\")\n",
        "\n",
        "        with tf.name_scope(\"accuracy\"):\n",
        "            self.correct_pred = tf.equal(tf.argmax(self.predictions, 1),tf.argmax(self.input_y, 1))\n",
        "            self.accuracy = tf.reduce_mean(tf.cast(self.correct_pred, \"float\"),name=\"accuracy\")\n",
        "\n",
        "        print (\"(!) LOADED CNN-LSTM! :)\")\n",
        "        #embed()\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fe1fy1VUvf1v",
        "colab_type": "text"
      },
      "source": [
        " **BATCHGEN FILE TO CREATE DATA FILES FOR TRAINING\n",
        "AND ALSO LOADING Global Vectors for embedding**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "px3IqUCRuC3W",
        "colab_type": "code",
        "outputId": "54c4d076-884a-4bf9-cd99-3df39064a640",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import csv\n",
        "import re\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "from IPython import embed\n",
        "\n",
        "#Separates a file with mixed positive and negative examples into two.\n",
        "def separate_dataset(filename):\n",
        "    good_out = open(\"good_\"+filename,\"w+\");\n",
        "    bad_out  = open(\"bad_\"+filename,\"w+\");\n",
        "\n",
        "    seen = 1;\n",
        "    with open('Sentiment Analysis Dataset.csv','r') as f:\n",
        "        reader = csv.reader(f)\n",
        "        next(reader)\n",
        "\n",
        "        for line in reader:\n",
        "            seen +=1\n",
        "            sentiment = line[1]\n",
        "            sentence = line[3]\n",
        "\n",
        "            if (sentiment == \"0\"):\n",
        "                bad_out.write(sentence+\"\\n\")\n",
        "            else:\n",
        "                good_out.write(sentence+\"\\n\")\n",
        "\n",
        "            if (seen%10000==0):\n",
        "                print (seen);\n",
        "\n",
        "    good_out.close();\n",
        "    bad_out.close();\n",
        "\n",
        "\n",
        "\n",
        "#Load Dataset\n",
        "def get_dataset(goodfile,badfile,limit,randomize=True):\n",
        "    good_x = list(open(goodfile,\"r\").readlines())\n",
        "    good_x = [s.strip() for s in good_x]\n",
        "    \n",
        "    bad_x  = list(open(badfile,\"r\").readlines())\n",
        "    bad_x  = [s.strip() for s in bad_x]\n",
        "\n",
        "    if (randomize):\n",
        "        random.shuffle(bad_x)\n",
        "        random.shuffle(good_x)\n",
        "\n",
        "    good_x = good_x[:limit]\n",
        "    bad_x = bad_x[:limit]\n",
        "\n",
        "    x = good_x + bad_x\n",
        "    x = [clean_str(s) for s in x]\n",
        "\n",
        "\n",
        "    positive_labels = [[0, 1] for _ in good_x]\n",
        "    negative_labels = [[1, 0] for _ in bad_x]\n",
        "    y = np.concatenate([positive_labels, negative_labels], 0)\n",
        "    return [x,y]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Clean Dataset\n",
        "def clean_str(string):\n",
        "\n",
        "\n",
        "    #EMOJIS\n",
        "    string = re.sub(r\":\\)\",\"emojihappy1\",string)\n",
        "    string = re.sub(r\":P\",\"emojihappy2\",string)\n",
        "    string = re.sub(r\":p\",\"emojihappy3\",string)\n",
        "    string = re.sub(r\":>\",\"emojihappy4\",string)\n",
        "    string = re.sub(r\":3\",\"emojihappy5\",string)\n",
        "    string = re.sub(r\":D\",\"emojihappy6\",string)\n",
        "    string = re.sub(r\" XD \",\"emojihappy7\",string)\n",
        "    string = re.sub(r\" <3 \",\"emojihappy8\",string)\n",
        "\n",
        "    string = re.sub(r\":\\(\",\"emojisad9\",string)\n",
        "    string = re.sub(r\":<\",\"emojisad10\",string)\n",
        "    string = re.sub(r\":<\",\"emojisad11\",string)\n",
        "    string = re.sub(r\">:\\(\",\"emojisad12\",string)\n",
        "\n",
        "    #MENTIONS \"(@)\\w+\"\n",
        "    string = re.sub(r\"(@)\\w+\",\"mentiontoken\",string)\n",
        "    \n",
        "    #WEBSITES\n",
        "    string = re.sub(r\"http(s)*:(\\S)*\",\"linktoken\",string)\n",
        "\n",
        "    #STRANGE UNICODE \\x...\n",
        "    string = re.sub(r\"\\\\x(\\S)*\",\"\",string)\n",
        "\n",
        "    #General Cleanup and Symbols\n",
        "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
        "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
        "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
        "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
        "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
        "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
        "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
        "    string = re.sub(r\",\", \" , \", string)\n",
        "    string = re.sub(r\"!\", \" ! \", string)\n",
        "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
        "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
        "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
        "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
        "\n",
        "    return string.strip().lower()\n",
        "\n",
        "\n",
        "\n",
        "#Generate random batches\n",
        "#Source: https://github.com/dennybritz/cnn-text-classification-tf/blob/master/data_helpers.py\n",
        "def gen_batch(data, batch_size, num_epochs, shuffle=True):\n",
        "    \"\"\"\n",
        "    Generates a batch iterator for a dataset.\n",
        "    \"\"\"\n",
        "    data = np.array(data)\n",
        "    data_size = len(data)\n",
        "    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
        "    for epoch in range(num_epochs):\n",
        "        # Shuffle the data at each epoch\n",
        "        if shuffle:\n",
        "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
        "            shuffled_data = data[shuffle_indices]\n",
        "        else:\n",
        "            shuffled_data = data\n",
        "        for batch_num in range(num_batches_per_epoch):\n",
        "            start_index = batch_num * batch_size\n",
        "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
        "            yield shuffled_data[start_index:end_index]\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    separate_dataset(\"small.txt\");\n",
        "\n",
        "\n",
        "#42\n",
        "#642\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000\n",
            "20000\n",
            "30000\n",
            "40000\n",
            "50000\n",
            "60000\n",
            "70000\n",
            "80000\n",
            "90000\n",
            "100000\n",
            "110000\n",
            "120000\n",
            "130000\n",
            "140000\n",
            "150000\n",
            "160000\n",
            "170000\n",
            "180000\n",
            "190000\n",
            "200000\n",
            "210000\n",
            "220000\n",
            "230000\n",
            "240000\n",
            "250000\n",
            "260000\n",
            "270000\n",
            "280000\n",
            "290000\n",
            "300000\n",
            "310000\n",
            "320000\n",
            "330000\n",
            "340000\n",
            "350000\n",
            "360000\n",
            "370000\n",
            "380000\n",
            "390000\n",
            "400000\n",
            "410000\n",
            "420000\n",
            "430000\n",
            "440000\n",
            "450000\n",
            "460000\n",
            "470000\n",
            "480000\n",
            "490000\n",
            "500000\n",
            "510000\n",
            "520000\n",
            "530000\n",
            "540000\n",
            "550000\n",
            "560000\n",
            "570000\n",
            "580000\n",
            "590000\n",
            "600000\n",
            "610000\n",
            "620000\n",
            "630000\n",
            "640000\n",
            "650000\n",
            "660000\n",
            "670000\n",
            "680000\n",
            "690000\n",
            "700000\n",
            "710000\n",
            "720000\n",
            "730000\n",
            "740000\n",
            "750000\n",
            "760000\n",
            "770000\n",
            "780000\n",
            "790000\n",
            "800000\n",
            "810000\n",
            "820000\n",
            "830000\n",
            "840000\n",
            "850000\n",
            "860000\n",
            "870000\n",
            "880000\n",
            "890000\n",
            "900000\n",
            "910000\n",
            "920000\n",
            "930000\n",
            "940000\n",
            "950000\n",
            "960000\n",
            "970000\n",
            "980000\n",
            "990000\n",
            "1000000\n",
            "1010000\n",
            "1020000\n",
            "1030000\n",
            "1040000\n",
            "1050000\n",
            "1060000\n",
            "1070000\n",
            "1080000\n",
            "1090000\n",
            "1100000\n",
            "1110000\n",
            "1120000\n",
            "1130000\n",
            "1140000\n",
            "1150000\n",
            "1160000\n",
            "1170000\n",
            "1180000\n",
            "1190000\n",
            "1200000\n",
            "1210000\n",
            "1220000\n",
            "1230000\n",
            "1240000\n",
            "1250000\n",
            "1260000\n",
            "1270000\n",
            "1280000\n",
            "1290000\n",
            "1300000\n",
            "1310000\n",
            "1320000\n",
            "1330000\n",
            "1340000\n",
            "1350000\n",
            "1360000\n",
            "1370000\n",
            "1380000\n",
            "1390000\n",
            "1400000\n",
            "1410000\n",
            "1420000\n",
            "1430000\n",
            "1440000\n",
            "1450000\n",
            "1460000\n",
            "1470000\n",
            "1480000\n",
            "1490000\n",
            "1500000\n",
            "1510000\n",
            "1520000\n",
            "1530000\n",
            "1540000\n",
            "1550000\n",
            "1560000\n",
            "1570000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KHQil_wvsjT",
        "colab_type": "text"
      },
      "source": [
        " **TRAINING THE DATA USING THE DATA GENERATED FROM ABOVE \n",
        "PS: Training here is done using the parameter given in the paper**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAe33yXhuH4f",
        "colab_type": "code",
        "outputId": "edde1dfd-aea5-4e54-ca1d-26ca73b9f6ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#! /usr/bin/env python\n",
        "import sys\n",
        "\n",
        "#SELECT WHICH MODEL YOU WISH TO RUN:\n",
        "from cnn_lstm import CNN_LSTM   #OPTION 0\n",
        "from lstm_cnn import LSTM_CNN   #OPTION 1\n",
        "from cnn import CNN             #OPTION 2 (Model by: Danny Britz)\n",
        "from lstm import LSTM           #OPTION 3\n",
        "MODEL_TO_RUN = 0\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import datetime\n",
        "import batchgen\n",
        "from tensorflow.contrib import learn\n",
        "\n",
        "from IPython import embed\n",
        "\n",
        "# Parameters\n",
        "# ==================================================\n",
        "\n",
        "# Data loading params\n",
        "dev_size = .10\n",
        "\n",
        "# Model Hyperparameters\n",
        "embedding_dim  = 32     #128\n",
        "max_seq_legth = 70 \n",
        "filter_sizes = [3,4,5]  #3\n",
        "num_filters = 32\n",
        "dropout_prob = 0.5 #0.5\n",
        "l2_reg_lambda = 0.0\n",
        "use_glove = True #Do we use glove\n",
        "\n",
        "# Training parameters\n",
        "batch_size = 128\n",
        "num_epochs = 10 #200\n",
        "evaluate_every = 100 #100\n",
        "checkpoint_every = 100000 #100\n",
        "num_checkpoints = 0 #Checkpoints to store\n",
        "\n",
        "\n",
        "# Misc Parameters\n",
        "allow_soft_placement = True\n",
        "log_device_placement = False\n",
        "\n",
        "\n",
        "\n",
        "# Data Preparation\n",
        "# ==================================================\n",
        "\n",
        "\n",
        "filename = \"Sentiment Analysis Dataset.csv\"\n",
        "goodfile = \"good_small.txt\"\n",
        "badfile = \"bad_small.txt\"\n",
        "\n",
        "\n",
        "# Load data\n",
        "print(\"Loading data...\")\n",
        "x_text, y = batchgen.get_dataset(goodfile, badfile, 5000) #TODO: MAX LENGTH\n",
        "\n",
        "# Build vocabulary\n",
        "max_document_length = max([len(x.split(\" \")) for x in x_text])\n",
        "if(not use_glove):\n",
        "    print (\"Not using GloVe\")\n",
        "    vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
        "    x = np.array(list(vocab_processor.fit_transform(x_text)))\n",
        "else:\n",
        "    print (\"Using GloVe\")\n",
        "    embedding_dim = 50\n",
        "    filename = './glove.6B.50d.txt'\n",
        "    def loadGloVe(filename):\n",
        "        vocab = []\n",
        "        embd = []\n",
        "        file = open(filename,'r')\n",
        "        for line in file.readlines():\n",
        "            row = line.strip().split(' ')\n",
        "            vocab.append(row[0])\n",
        "            embd.append(row[1:])\n",
        "        print('Loaded GloVe!')\n",
        "        file.close()\n",
        "        return vocab,embd\n",
        "    vocab,embd = loadGloVe(filename)\n",
        "    vocab_size = len(vocab)\n",
        "    embedding_dim = len(embd[0])\n",
        "    embedding = np.asarray(embd)\n",
        "\n",
        "    W = tf.Variable(tf.constant(0.0, shape=[vocab_size, embedding_dim]),\n",
        "                    trainable=False, name=\"W\")\n",
        "    embedding_placeholder = tf.placeholder(tf.float32, [vocab_size, embedding_dim])\n",
        "    embedding_init = W.assign(embedding_placeholder)\n",
        "\n",
        "    session_conf = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
        "    sess = tf.Session(config=session_conf)\n",
        "    sess.run(embedding_init, feed_dict={embedding_placeholder: embedding})\n",
        "\n",
        "    from tensorflow.contrib import learn\n",
        "    #init vocab processor\n",
        "    vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
        "    #fit the vocab from glove\n",
        "    pretrain = vocab_processor.fit(vocab)\n",
        "    #transform inputs\n",
        "    x = np.array(list(vocab_processor.transform(x_text)))\n",
        "\n",
        "    #init vocab processor\n",
        "    vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
        "    #fit the vocab from glove\n",
        "    pretrain = vocab_processor.fit(vocab)\n",
        "    #transform inputs\n",
        "    x = np.array(list(vocab_processor.transform(x_text)))\n",
        "\n",
        "\n",
        "# Randomly shuffle data\n",
        "np.random.seed(42)\n",
        "shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
        "x_shuffled = x[shuffle_indices]\n",
        "y_shuffled = y[shuffle_indices]\n",
        "\n",
        "# Split train/test set\n",
        "# TODO: This is very crude, should use cross-validation\n",
        "dev_sample_index = -1 * int(dev_size * float(len(y)))\n",
        "x_train, x_dev = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]\n",
        "y_train, y_dev = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]\n",
        "print(\"Vocabulary Size: {:d}\".format(len(vocab_processor.vocabulary_)))\n",
        "print(\"Train/Dev split: {:d}/{:d}\".format(len(y_train), len(y_dev)))\n",
        "\n",
        "#embed()\n",
        "\n",
        "\n",
        "# Training\n",
        "# ==================================================\n",
        "\n",
        "with tf.Graph().as_default():\n",
        "    session_conf = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
        "    sess = tf.Session(config=session_conf)\n",
        "    with sess.as_default():\n",
        "        #embed()\n",
        "        if (MODEL_TO_RUN == 0):\n",
        "            model = CNN_LSTM(x_train.shape[1],y_train.shape[1],len(vocab_processor.vocabulary_),embedding_dim,filter_sizes,num_filters,l2_reg_lambda)\n",
        "        elif (MODEL_TO_RUN == 1):\n",
        "            model = LSTM_CNN(x_train.shape[1],y_train.shape[1],len(vocab_processor.vocabulary_),embedding_dim,filter_sizes,num_filters,l2_reg_lambda)\n",
        "        elif (MODEL_TO_RUN == 2):\n",
        "            model = CNN(x_train.shape[1],y_train.shape[1],len(vocab_processor.vocabulary_),embedding_dim,filter_sizes,num_filters,l2_reg_lambda)\n",
        "        elif (MODEL_TO_RUN == 3):\n",
        "            model = LSTM(x_train.shape[1],y_train.shape[1],len(vocab_processor.vocabulary_),embedding_dim)\n",
        "        else:\n",
        "            print (\"PLEASE CHOOSE A VALID MODEL!\\n0 = CNN_LSTM\\n1 = LSTM_CNN\\n2 = CNN\\n3 = LSTM\\n\")\n",
        "            exit();\n",
        "\n",
        "\n",
        "        # Define Training procedure\n",
        "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
        "        optimizer = tf.train.AdamOptimizer(1e-3)\n",
        "        grads_and_vars = optimizer.compute_gradients(model.loss)\n",
        "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
        "\n",
        "        # Keep track of gradient values and sparsity (optional)\n",
        "        grad_summaries = []\n",
        "        for g, v in grads_and_vars:\n",
        "            if g is not None:\n",
        "                grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
        "                sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
        "                grad_summaries.append(grad_hist_summary)\n",
        "                grad_summaries.append(sparsity_summary)\n",
        "        grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
        "\n",
        "        # Output directory for models and summaries\n",
        "        timestamp = str(int(time.time()))\n",
        "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
        "        print(\"Writing to {}\\n\".format(out_dir))\n",
        "\n",
        "        # Summaries for loss and accuracy\n",
        "        loss_summary = tf.summary.scalar(\"loss\", model.loss)\n",
        "        acc_summary = tf.summary.scalar(\"accuracy\", model.accuracy)\n",
        "\n",
        "        # Train Summaries\n",
        "        train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
        "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
        "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
        "\n",
        "        # Dev summaries\n",
        "        dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
        "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
        "        dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
        "\n",
        "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
        "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
        "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
        "        if not os.path.exists(checkpoint_dir):\n",
        "            os.makedirs(checkpoint_dir)\n",
        "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=num_checkpoints)\n",
        "\n",
        "        # Write vocabulary\n",
        "        vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
        "\n",
        "        # Initialize all variables\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "\n",
        "        #TRAINING STEP\n",
        "        def train_step(x_batch, y_batch,save=False):\n",
        "            feed_dict = {\n",
        "              model.input_x: x_batch,\n",
        "              model.input_y: y_batch,\n",
        "              model.dropout_keep_prob: dropout_prob\n",
        "            }\n",
        "            _, step, summaries, loss, accuracy = sess.run(\n",
        "                [train_op, global_step, train_summary_op, model.loss, model.accuracy],\n",
        "                feed_dict)\n",
        "            time_str = datetime.datetime.now().isoformat()\n",
        "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
        "            if save:\n",
        "                train_summary_writer.add_summary(summaries, step)\n",
        "\n",
        "        #EVALUATE MODEL\n",
        "        def dev_step(x_batch, y_batch, writer=None,save=False):\n",
        "            feed_dict = {\n",
        "              model.input_x: x_batch,\n",
        "              model.input_y: y_batch,\n",
        "              model.dropout_keep_prob: 0.5\n",
        "            }\n",
        "            step, summaries, loss, accuracy = sess.run(\n",
        "                [global_step, dev_summary_op, model.loss, model.accuracy],\n",
        "                feed_dict)\n",
        "            time_str = datetime.datetime.now().isoformat()\n",
        "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
        "            if save:\n",
        "                if writer:\n",
        "                    writer.add_summary(summaries, step)\n",
        "\n",
        "        #CREATE THE BATCHES GENERATOR\n",
        "        batches = batchgen.gen_batch(list(zip(x_train, y_train)), batch_size, num_epochs)\n",
        "        \n",
        "        #TRAIN FOR EACH BATCH\n",
        "        for batch in batches:\n",
        "            x_batch, y_batch = zip(*batch)\n",
        "            train_step(x_batch, y_batch)\n",
        "            current_step = tf.train.global_step(sess, global_step)\n",
        "            if current_step % evaluate_every == 0:\n",
        "                print(\"\\nEvaluation:\")\n",
        "                dev_step(x_dev, y_dev, writer=dev_summary_writer)\n",
        "                print(\"\")\n",
        "            if current_step % checkpoint_every == 0:\n",
        "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
        "                print(\"Saved model checkpoint to {}\\n\".format(path))\n",
        "        dev_step(x_dev, y_dev, writer=dev_summary_writer)\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "Using GloVe\n",
            "Loaded GloVe!\n",
            "WARNING:tensorflow:From <ipython-input-5-12f5c36257a8>:100: VocabularyProcessor.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tensorflow/transform or tf.data.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/preprocessing/text.py:154: CategoricalVocabulary.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tensorflow/transform or tf.data.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/preprocessing/text.py:170: tokenizer (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tensorflow/transform or tf.data.\n",
            "Vocabulary Size: 370847\n",
            "Train/Dev split: 9000/1000\n",
            "WARNING:tensorflow:From /content/gdrive/My Drive/CS291K-master/cnn_lstm.py:18: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gdrive/My Drive/CS291K-master/cnn_lstm.py:28: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gdrive/My Drive/CS291K-master/cnn_lstm.py:34: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gdrive/My Drive/CS291K-master/cnn_lstm.py:44: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /content/gdrive/My Drive/CS291K-master/cnn_lstm.py:47: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From /content/gdrive/My Drive/CS291K-master/cnn_lstm.py:49: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/rnn_cell_impl.py:958: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.add_weight` method instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/rnn_cell_impl.py:962: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /content/gdrive/My Drive/CS291K-master/cnn_lstm.py:56: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gdrive/My Drive/CS291K-master/cnn_lstm.py:62: The name tf.nn.xw_plus_b is deprecated. Please use tf.compat.v1.nn.xw_plus_b instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gdrive/My Drive/CS291K-master/cnn_lstm.py:66: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "(!) LOADED CNN-LSTM! :)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
            "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
            "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/hist is illegal; using conv-maxpool-3/W_0/grad/hist instead.\n",
            "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/sparsity is illegal; using conv-maxpool-3/W_0/grad/sparsity instead.\n",
            "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/hist is illegal; using conv-maxpool-3/b_0/grad/hist instead.\n",
            "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/sparsity is illegal; using conv-maxpool-3/b_0/grad/sparsity instead.\n",
            "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/hist is illegal; using conv-maxpool-4/W_0/grad/hist instead.\n",
            "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/sparsity is illegal; using conv-maxpool-4/W_0/grad/sparsity instead.\n",
            "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/hist is illegal; using conv-maxpool-4/b_0/grad/hist instead.\n",
            "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/sparsity is illegal; using conv-maxpool-4/b_0/grad/sparsity instead.\n",
            "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/hist is illegal; using conv-maxpool-5/W_0/grad/hist instead.\n",
            "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/sparsity is illegal; using conv-maxpool-5/W_0/grad/sparsity instead.\n",
            "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/hist is illegal; using conv-maxpool-5/b_0/grad/hist instead.\n",
            "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/sparsity is illegal; using conv-maxpool-5/b_0/grad/sparsity instead.\n",
            "INFO:tensorflow:Summary name rnn/lstm_cell/kernel:0/grad/hist is illegal; using rnn/lstm_cell/kernel_0/grad/hist instead.\n",
            "INFO:tensorflow:Summary name rnn/lstm_cell/kernel:0/grad/sparsity is illegal; using rnn/lstm_cell/kernel_0/grad/sparsity instead.\n",
            "INFO:tensorflow:Summary name rnn/lstm_cell/bias:0/grad/hist is illegal; using rnn/lstm_cell/bias_0/grad/hist instead.\n",
            "INFO:tensorflow:Summary name rnn/lstm_cell/bias:0/grad/sparsity is illegal; using rnn/lstm_cell/bias_0/grad/sparsity instead.\n",
            "INFO:tensorflow:Summary name Variable:0/grad/hist is illegal; using Variable_0/grad/hist instead.\n",
            "INFO:tensorflow:Summary name Variable:0/grad/sparsity is illegal; using Variable_0/grad/sparsity instead.\n",
            "INFO:tensorflow:Summary name Variable_1:0/grad/hist is illegal; using Variable_1_0/grad/hist instead.\n",
            "INFO:tensorflow:Summary name Variable_1:0/grad/sparsity is illegal; using Variable_1_0/grad/sparsity instead.\n",
            "Writing to /content/gdrive/My Drive/CS291K-master/runs/1572372011\n",
            "\n",
            "2019-10-29T18:00:13.958390: step 1, loss 1.23986, acc 0.421875\n",
            "2019-10-29T18:00:14.725478: step 2, loss 0.740794, acc 0.515625\n",
            "2019-10-29T18:00:15.462119: step 3, loss 0.723574, acc 0.601562\n",
            "2019-10-29T18:00:16.214022: step 4, loss 0.999782, acc 0.421875\n",
            "2019-10-29T18:00:16.943414: step 5, loss 0.950991, acc 0.453125\n",
            "2019-10-29T18:00:17.698178: step 6, loss 0.75756, acc 0.53125\n",
            "2019-10-29T18:00:18.451636: step 7, loss 0.725505, acc 0.53125\n",
            "2019-10-29T18:00:19.207610: step 8, loss 0.730965, acc 0.445312\n",
            "2019-10-29T18:00:19.940007: step 9, loss 0.722765, acc 0.492188\n",
            "2019-10-29T18:00:20.680738: step 10, loss 0.692145, acc 0.609375\n",
            "2019-10-29T18:00:21.418738: step 11, loss 0.732039, acc 0.53125\n",
            "2019-10-29T18:00:22.154492: step 12, loss 0.740995, acc 0.492188\n",
            "2019-10-29T18:00:22.886099: step 13, loss 0.76097, acc 0.492188\n",
            "2019-10-29T18:00:23.623899: step 14, loss 0.743724, acc 0.453125\n",
            "2019-10-29T18:00:24.355610: step 15, loss 0.680277, acc 0.578125\n",
            "2019-10-29T18:00:25.084656: step 16, loss 0.70531, acc 0.492188\n",
            "2019-10-29T18:00:25.820022: step 17, loss 0.707495, acc 0.46875\n",
            "2019-10-29T18:00:26.548196: step 18, loss 0.700869, acc 0.585938\n",
            "2019-10-29T18:00:27.275320: step 19, loss 0.72706, acc 0.4375\n",
            "2019-10-29T18:00:28.012131: step 20, loss 0.729854, acc 0.492188\n",
            "2019-10-29T18:00:28.738950: step 21, loss 0.713819, acc 0.476562\n",
            "2019-10-29T18:00:29.476986: step 22, loss 0.705908, acc 0.507812\n",
            "2019-10-29T18:00:30.217541: step 23, loss 0.678156, acc 0.578125\n",
            "2019-10-29T18:00:30.942416: step 24, loss 0.698896, acc 0.507812\n",
            "2019-10-29T18:00:31.668920: step 25, loss 0.691628, acc 0.507812\n",
            "2019-10-29T18:00:32.412257: step 26, loss 0.68855, acc 0.546875\n",
            "2019-10-29T18:00:33.163144: step 27, loss 0.698305, acc 0.476562\n",
            "2019-10-29T18:00:33.899153: step 28, loss 0.699595, acc 0.476562\n",
            "2019-10-29T18:00:34.636429: step 29, loss 0.694771, acc 0.484375\n",
            "2019-10-29T18:00:35.388480: step 30, loss 0.700951, acc 0.515625\n",
            "2019-10-29T18:00:36.114438: step 31, loss 0.7071, acc 0.492188\n",
            "2019-10-29T18:00:36.850915: step 32, loss 0.709711, acc 0.453125\n",
            "2019-10-29T18:00:37.590139: step 33, loss 0.696, acc 0.5\n",
            "2019-10-29T18:00:38.316821: step 34, loss 0.704688, acc 0.453125\n",
            "2019-10-29T18:00:39.048936: step 35, loss 0.684585, acc 0.546875\n",
            "2019-10-29T18:00:39.785843: step 36, loss 0.687989, acc 0.539062\n",
            "2019-10-29T18:00:40.514206: step 37, loss 0.687947, acc 0.539062\n",
            "2019-10-29T18:00:41.244310: step 38, loss 0.686139, acc 0.539062\n",
            "2019-10-29T18:00:41.978108: step 39, loss 0.693051, acc 0.515625\n",
            "2019-10-29T18:00:42.700689: step 40, loss 0.696632, acc 0.484375\n",
            "2019-10-29T18:00:43.428133: step 41, loss 0.686823, acc 0.53125\n",
            "2019-10-29T18:00:44.149338: step 42, loss 0.68492, acc 0.539062\n",
            "2019-10-29T18:00:44.874864: step 43, loss 0.696842, acc 0.492188\n",
            "2019-10-29T18:00:45.591068: step 44, loss 0.693383, acc 0.507812\n",
            "2019-10-29T18:00:46.303353: step 45, loss 0.690148, acc 0.507812\n",
            "2019-10-29T18:00:47.030969: step 46, loss 0.688984, acc 0.523438\n",
            "2019-10-29T18:00:47.756873: step 47, loss 0.705653, acc 0.40625\n",
            "2019-10-29T18:00:48.487104: step 48, loss 0.696629, acc 0.515625\n",
            "2019-10-29T18:00:49.233200: step 49, loss 0.685387, acc 0.625\n",
            "2019-10-29T18:00:49.967194: step 50, loss 0.687717, acc 0.507812\n",
            "2019-10-29T18:00:50.691302: step 51, loss 0.707988, acc 0.453125\n",
            "2019-10-29T18:00:51.421968: step 52, loss 0.681388, acc 0.539062\n",
            "2019-10-29T18:00:52.157975: step 53, loss 0.690309, acc 0.554688\n",
            "2019-10-29T18:00:52.900094: step 54, loss 0.698148, acc 0.492188\n",
            "2019-10-29T18:00:53.631722: step 55, loss 0.683602, acc 0.570312\n",
            "2019-10-29T18:00:54.362917: step 56, loss 0.688098, acc 0.554688\n",
            "2019-10-29T18:00:55.089446: step 57, loss 0.69744, acc 0.539062\n",
            "2019-10-29T18:00:55.822738: step 58, loss 0.677216, acc 0.59375\n",
            "2019-10-29T18:00:56.554588: step 59, loss 0.703489, acc 0.507812\n",
            "2019-10-29T18:00:57.384906: step 60, loss 0.700736, acc 0.507812\n",
            "2019-10-29T18:00:58.111120: step 61, loss 0.708531, acc 0.46875\n",
            "2019-10-29T18:00:58.840815: step 62, loss 0.690376, acc 0.492188\n",
            "2019-10-29T18:00:59.583408: step 63, loss 0.699998, acc 0.5\n",
            "2019-10-29T18:01:00.334253: step 64, loss 0.701062, acc 0.4375\n",
            "2019-10-29T18:01:01.073730: step 65, loss 0.693607, acc 0.507812\n",
            "2019-10-29T18:01:01.802254: step 66, loss 0.694378, acc 0.507812\n",
            "2019-10-29T18:01:02.539636: step 67, loss 0.697164, acc 0.523438\n",
            "2019-10-29T18:01:03.268869: step 68, loss 0.696724, acc 0.53125\n",
            "2019-10-29T18:01:04.006421: step 69, loss 0.678206, acc 0.617188\n",
            "2019-10-29T18:01:04.753307: step 70, loss 0.712691, acc 0.460938\n",
            "2019-10-29T18:01:05.345580: step 71, loss 0.71463, acc 0.35\n",
            "2019-10-29T18:01:06.116820: step 72, loss 0.692772, acc 0.53125\n",
            "2019-10-29T18:01:06.865162: step 73, loss 0.684453, acc 0.539062\n",
            "2019-10-29T18:01:07.588073: step 74, loss 0.686758, acc 0.570312\n",
            "2019-10-29T18:01:08.309594: step 75, loss 0.68621, acc 0.546875\n",
            "2019-10-29T18:01:09.045643: step 76, loss 0.694647, acc 0.570312\n",
            "2019-10-29T18:01:09.776261: step 77, loss 0.709886, acc 0.492188\n",
            "2019-10-29T18:01:10.514713: step 78, loss 0.697029, acc 0.492188\n",
            "2019-10-29T18:01:11.250315: step 79, loss 0.684957, acc 0.59375\n",
            "2019-10-29T18:01:11.979626: step 80, loss 0.685121, acc 0.5625\n",
            "2019-10-29T18:01:12.714254: step 81, loss 0.699309, acc 0.492188\n",
            "2019-10-29T18:01:13.447950: step 82, loss 0.690101, acc 0.445312\n",
            "2019-10-29T18:01:14.179096: step 83, loss 0.69015, acc 0.585938\n",
            "2019-10-29T18:01:14.907768: step 84, loss 0.686694, acc 0.554688\n",
            "2019-10-29T18:01:15.648285: step 85, loss 0.705142, acc 0.4375\n",
            "2019-10-29T18:01:16.386330: step 86, loss 0.688885, acc 0.546875\n",
            "2019-10-29T18:01:17.105399: step 87, loss 0.69603, acc 0.515625\n",
            "2019-10-29T18:01:17.834904: step 88, loss 0.682463, acc 0.554688\n",
            "2019-10-29T18:01:18.568783: step 89, loss 0.68968, acc 0.585938\n",
            "2019-10-29T18:01:19.305207: step 90, loss 0.692376, acc 0.484375\n",
            "2019-10-29T18:01:20.047838: step 91, loss 0.68659, acc 0.492188\n",
            "2019-10-29T18:01:20.787354: step 92, loss 0.68331, acc 0.585938\n",
            "2019-10-29T18:01:21.517356: step 93, loss 0.698773, acc 0.523438\n",
            "2019-10-29T18:01:22.243127: step 94, loss 0.689701, acc 0.554688\n",
            "2019-10-29T18:01:22.987724: step 95, loss 0.683534, acc 0.632812\n",
            "2019-10-29T18:01:23.717925: step 96, loss 0.678454, acc 0.617188\n",
            "2019-10-29T18:01:24.457069: step 97, loss 0.69972, acc 0.515625\n",
            "2019-10-29T18:01:25.181986: step 98, loss 0.708983, acc 0.476562\n",
            "2019-10-29T18:01:25.921392: step 99, loss 0.713667, acc 0.476562\n",
            "2019-10-29T18:01:26.666977: step 100, loss 0.706661, acc 0.484375\n",
            "\n",
            "Evaluation:\n",
            "2019-10-29T18:01:27.214116: step 100, loss 0.694169, acc 0.503\n",
            "\n",
            "2019-10-29T18:01:27.941885: step 101, loss 0.687162, acc 0.515625\n",
            "2019-10-29T18:01:28.676103: step 102, loss 0.679077, acc 0.5625\n",
            "2019-10-29T18:01:29.413182: step 103, loss 0.695292, acc 0.4375\n",
            "2019-10-29T18:01:30.149908: step 104, loss 0.680301, acc 0.546875\n",
            "2019-10-29T18:01:30.901583: step 105, loss 0.687647, acc 0.53125\n",
            "2019-10-29T18:01:31.631034: step 106, loss 0.70065, acc 0.5\n",
            "2019-10-29T18:01:32.367584: step 107, loss 0.689696, acc 0.554688\n",
            "2019-10-29T18:01:33.104747: step 108, loss 0.71021, acc 0.460938\n",
            "2019-10-29T18:01:33.830200: step 109, loss 0.689031, acc 0.554688\n",
            "2019-10-29T18:01:34.566323: step 110, loss 0.698617, acc 0.476562\n",
            "2019-10-29T18:01:35.295418: step 111, loss 0.702201, acc 0.460938\n",
            "2019-10-29T18:01:36.029760: step 112, loss 0.691453, acc 0.546875\n",
            "2019-10-29T18:01:36.765171: step 113, loss 0.696574, acc 0.492188\n",
            "2019-10-29T18:01:37.488444: step 114, loss 0.690495, acc 0.523438\n",
            "2019-10-29T18:01:38.209992: step 115, loss 0.69312, acc 0.539062\n",
            "2019-10-29T18:01:38.941158: step 116, loss 0.692545, acc 0.546875\n",
            "2019-10-29T18:01:39.681548: step 117, loss 0.703887, acc 0.515625\n",
            "2019-10-29T18:01:40.426946: step 118, loss 0.702982, acc 0.507812\n",
            "2019-10-29T18:01:41.155285: step 119, loss 0.692626, acc 0.546875\n",
            "2019-10-29T18:01:41.889820: step 120, loss 0.669609, acc 0.59375\n",
            "2019-10-29T18:01:42.615392: step 121, loss 0.697516, acc 0.484375\n",
            "2019-10-29T18:01:43.331583: step 122, loss 0.682285, acc 0.570312\n",
            "2019-10-29T18:01:44.055337: step 123, loss 0.695616, acc 0.523438\n",
            "2019-10-29T18:01:44.785325: step 124, loss 0.683464, acc 0.5625\n",
            "2019-10-29T18:01:45.518242: step 125, loss 0.700094, acc 0.46875\n",
            "2019-10-29T18:01:46.268302: step 126, loss 0.677524, acc 0.554688\n",
            "2019-10-29T18:01:47.007003: step 127, loss 0.678138, acc 0.570312\n",
            "2019-10-29T18:01:47.746381: step 128, loss 0.705441, acc 0.46875\n",
            "2019-10-29T18:01:48.486995: step 129, loss 0.696016, acc 0.476562\n",
            "2019-10-29T18:01:49.232745: step 130, loss 0.686612, acc 0.570312\n",
            "2019-10-29T18:01:49.967574: step 131, loss 0.672438, acc 0.59375\n",
            "2019-10-29T18:01:50.712682: step 132, loss 0.695035, acc 0.507812\n",
            "2019-10-29T18:01:51.440864: step 133, loss 0.703577, acc 0.515625\n",
            "2019-10-29T18:01:52.188677: step 134, loss 0.694481, acc 0.515625\n",
            "2019-10-29T18:01:52.915674: step 135, loss 0.688082, acc 0.5625\n",
            "2019-10-29T18:01:53.684881: step 136, loss 0.709986, acc 0.460938\n",
            "2019-10-29T18:01:54.419163: step 137, loss 0.686364, acc 0.5625\n",
            "2019-10-29T18:01:55.151559: step 138, loss 0.697281, acc 0.5\n",
            "2019-10-29T18:01:55.886369: step 139, loss 0.692026, acc 0.53125\n",
            "2019-10-29T18:01:56.624635: step 140, loss 0.701352, acc 0.507812\n",
            "2019-10-29T18:01:57.354103: step 141, loss 0.695516, acc 0.515625\n",
            "2019-10-29T18:01:57.934208: step 142, loss 0.673064, acc 0.675\n",
            "2019-10-29T18:01:58.701183: step 143, loss 0.669527, acc 0.546875\n",
            "2019-10-29T18:01:59.426327: step 144, loss 0.697997, acc 0.492188\n",
            "2019-10-29T18:02:00.150513: step 145, loss 0.67777, acc 0.585938\n",
            "2019-10-29T18:02:00.879697: step 146, loss 0.696169, acc 0.492188\n",
            "2019-10-29T18:02:01.616767: step 147, loss 0.683249, acc 0.578125\n",
            "2019-10-29T18:02:02.350099: step 148, loss 0.687568, acc 0.515625\n",
            "2019-10-29T18:02:03.074552: step 149, loss 0.663575, acc 0.648438\n",
            "2019-10-29T18:02:03.803018: step 150, loss 0.692739, acc 0.46875\n",
            "2019-10-29T18:02:04.542549: step 151, loss 0.678547, acc 0.59375\n",
            "2019-10-29T18:02:05.266271: step 152, loss 0.678119, acc 0.59375\n",
            "2019-10-29T18:02:06.006906: step 153, loss 0.679596, acc 0.546875\n",
            "2019-10-29T18:02:06.746612: step 154, loss 0.701155, acc 0.507812\n",
            "2019-10-29T18:02:07.487261: step 155, loss 0.691337, acc 0.5625\n",
            "2019-10-29T18:02:08.224838: step 156, loss 0.689956, acc 0.554688\n",
            "2019-10-29T18:02:08.956372: step 157, loss 0.695973, acc 0.5\n",
            "2019-10-29T18:02:09.693152: step 158, loss 0.691173, acc 0.484375\n",
            "2019-10-29T18:02:10.420564: step 159, loss 0.675331, acc 0.5625\n",
            "2019-10-29T18:02:11.149551: step 160, loss 0.689782, acc 0.539062\n",
            "2019-10-29T18:02:11.874193: step 161, loss 0.660679, acc 0.617188\n",
            "2019-10-29T18:02:12.617945: step 162, loss 0.688975, acc 0.515625\n",
            "2019-10-29T18:02:13.348104: step 163, loss 0.681168, acc 0.609375\n",
            "2019-10-29T18:02:14.081645: step 164, loss 0.692204, acc 0.523438\n",
            "2019-10-29T18:02:14.812958: step 165, loss 0.726096, acc 0.484375\n",
            "2019-10-29T18:02:15.546521: step 166, loss 0.689756, acc 0.53125\n",
            "2019-10-29T18:02:16.300814: step 167, loss 0.691301, acc 0.53125\n",
            "2019-10-29T18:02:17.033527: step 168, loss 0.676808, acc 0.585938\n",
            "2019-10-29T18:02:17.753881: step 169, loss 0.682581, acc 0.5\n",
            "2019-10-29T18:02:18.485379: step 170, loss 0.673916, acc 0.554688\n",
            "2019-10-29T18:02:19.209784: step 171, loss 0.657614, acc 0.578125\n",
            "2019-10-29T18:02:19.932188: step 172, loss 0.684334, acc 0.53125\n",
            "2019-10-29T18:02:20.663894: step 173, loss 0.700316, acc 0.539062\n",
            "2019-10-29T18:02:21.404657: step 174, loss 0.689384, acc 0.5625\n",
            "2019-10-29T18:02:22.135881: step 175, loss 0.696608, acc 0.539062\n",
            "2019-10-29T18:02:22.874451: step 176, loss 0.661237, acc 0.5625\n",
            "2019-10-29T18:02:23.609917: step 177, loss 0.681482, acc 0.554688\n",
            "2019-10-29T18:02:24.350350: step 178, loss 0.655606, acc 0.585938\n",
            "2019-10-29T18:02:25.086235: step 179, loss 0.665556, acc 0.625\n",
            "2019-10-29T18:02:25.811016: step 180, loss 0.717186, acc 0.546875\n",
            "2019-10-29T18:02:26.548274: step 181, loss 0.723339, acc 0.515625\n",
            "2019-10-29T18:02:27.312292: step 182, loss 0.670456, acc 0.585938\n",
            "2019-10-29T18:02:28.078269: step 183, loss 0.689684, acc 0.578125\n",
            "2019-10-29T18:02:28.851093: step 184, loss 0.695649, acc 0.53125\n",
            "2019-10-29T18:02:29.613153: step 185, loss 0.668685, acc 0.546875\n",
            "2019-10-29T18:02:30.372942: step 186, loss 0.691834, acc 0.476562\n",
            "2019-10-29T18:02:31.124122: step 187, loss 0.731474, acc 0.484375\n",
            "2019-10-29T18:02:31.872197: step 188, loss 0.681796, acc 0.554688\n",
            "2019-10-29T18:02:32.625002: step 189, loss 0.685155, acc 0.554688\n",
            "2019-10-29T18:02:33.384663: step 190, loss 0.656163, acc 0.632812\n",
            "2019-10-29T18:02:34.134598: step 191, loss 0.690094, acc 0.515625\n",
            "2019-10-29T18:02:34.892637: step 192, loss 0.682265, acc 0.585938\n",
            "2019-10-29T18:02:35.660862: step 193, loss 0.683788, acc 0.5625\n",
            "2019-10-29T18:02:36.427106: step 194, loss 0.670578, acc 0.578125\n",
            "2019-10-29T18:02:37.199309: step 195, loss 0.67508, acc 0.578125\n",
            "2019-10-29T18:02:37.927965: step 196, loss 0.718115, acc 0.507812\n",
            "2019-10-29T18:02:38.646062: step 197, loss 0.683733, acc 0.578125\n",
            "2019-10-29T18:02:39.378962: step 198, loss 0.68553, acc 0.554688\n",
            "2019-10-29T18:02:40.128427: step 199, loss 0.648287, acc 0.648438\n",
            "2019-10-29T18:02:40.882683: step 200, loss 0.669258, acc 0.570312\n",
            "\n",
            "Evaluation:\n",
            "2019-10-29T18:02:41.461590: step 200, loss 0.704919, acc 0.523\n",
            "\n",
            "2019-10-29T18:02:42.190701: step 201, loss 0.691716, acc 0.515625\n",
            "2019-10-29T18:02:42.922657: step 202, loss 0.667287, acc 0.601562\n",
            "2019-10-29T18:02:43.646440: step 203, loss 0.704962, acc 0.546875\n",
            "2019-10-29T18:02:44.368501: step 204, loss 0.687139, acc 0.5625\n",
            "2019-10-29T18:02:45.093643: step 205, loss 0.682182, acc 0.5625\n",
            "2019-10-29T18:02:45.827356: step 206, loss 0.67223, acc 0.515625\n",
            "2019-10-29T18:02:46.561120: step 207, loss 0.69148, acc 0.570312\n",
            "2019-10-29T18:02:47.284157: step 208, loss 0.70001, acc 0.554688\n",
            "2019-10-29T18:02:48.006735: step 209, loss 0.620981, acc 0.640625\n",
            "2019-10-29T18:02:48.738261: step 210, loss 0.673239, acc 0.59375\n",
            "2019-10-29T18:02:49.469602: step 211, loss 0.650309, acc 0.609375\n",
            "2019-10-29T18:02:50.192400: step 212, loss 0.638475, acc 0.671875\n",
            "2019-10-29T18:02:50.781716: step 213, loss 0.686523, acc 0.55\n",
            "2019-10-29T18:02:51.533930: step 214, loss 0.634627, acc 0.617188\n",
            "2019-10-29T18:02:52.265430: step 215, loss 0.626901, acc 0.671875\n",
            "2019-10-29T18:02:52.986005: step 216, loss 0.668392, acc 0.617188\n",
            "2019-10-29T18:02:53.714614: step 217, loss 0.714143, acc 0.546875\n",
            "2019-10-29T18:02:54.469090: step 218, loss 0.63634, acc 0.625\n",
            "2019-10-29T18:02:55.195914: step 219, loss 0.66389, acc 0.585938\n",
            "2019-10-29T18:02:55.931543: step 220, loss 0.632094, acc 0.632812\n",
            "2019-10-29T18:02:56.655787: step 221, loss 0.666282, acc 0.640625\n",
            "2019-10-29T18:02:57.381455: step 222, loss 0.653745, acc 0.570312\n",
            "2019-10-29T18:02:58.110072: step 223, loss 0.636498, acc 0.664062\n",
            "2019-10-29T18:02:58.847528: step 224, loss 0.655117, acc 0.609375\n",
            "2019-10-29T18:02:59.593893: step 225, loss 0.646083, acc 0.617188\n",
            "2019-10-29T18:03:00.330820: step 226, loss 0.648703, acc 0.59375\n",
            "2019-10-29T18:03:01.057384: step 227, loss 0.648278, acc 0.601562\n",
            "2019-10-29T18:03:01.780950: step 228, loss 0.64267, acc 0.617188\n",
            "2019-10-29T18:03:02.512724: step 229, loss 0.625385, acc 0.65625\n",
            "2019-10-29T18:03:03.243567: step 230, loss 0.608015, acc 0.664062\n",
            "2019-10-29T18:03:03.973205: step 231, loss 0.607078, acc 0.671875\n",
            "2019-10-29T18:03:04.702752: step 232, loss 0.667994, acc 0.609375\n",
            "2019-10-29T18:03:05.448752: step 233, loss 0.6456, acc 0.59375\n",
            "2019-10-29T18:03:06.186585: step 234, loss 0.656022, acc 0.59375\n",
            "2019-10-29T18:03:06.925797: step 235, loss 0.680984, acc 0.601562\n",
            "2019-10-29T18:03:07.658793: step 236, loss 0.670446, acc 0.585938\n",
            "2019-10-29T18:03:08.402905: step 237, loss 0.694084, acc 0.570312\n",
            "2019-10-29T18:03:09.130392: step 238, loss 0.678365, acc 0.601562\n",
            "2019-10-29T18:03:09.854434: step 239, loss 0.659999, acc 0.59375\n",
            "2019-10-29T18:03:10.605846: step 240, loss 0.61134, acc 0.648438\n",
            "2019-10-29T18:03:11.333187: step 241, loss 0.648846, acc 0.601562\n",
            "2019-10-29T18:03:12.068903: step 242, loss 0.632291, acc 0.695312\n",
            "2019-10-29T18:03:12.796046: step 243, loss 0.62365, acc 0.695312\n",
            "2019-10-29T18:03:13.521858: step 244, loss 0.615503, acc 0.757812\n",
            "2019-10-29T18:03:14.263109: step 245, loss 0.631317, acc 0.632812\n",
            "2019-10-29T18:03:14.989967: step 246, loss 0.666332, acc 0.578125\n",
            "2019-10-29T18:03:15.731143: step 247, loss 0.627055, acc 0.703125\n",
            "2019-10-29T18:03:16.455344: step 248, loss 0.668709, acc 0.59375\n",
            "2019-10-29T18:03:17.185675: step 249, loss 0.632121, acc 0.640625\n",
            "2019-10-29T18:03:17.918901: step 250, loss 0.640983, acc 0.632812\n",
            "2019-10-29T18:03:18.640732: step 251, loss 0.700383, acc 0.554688\n",
            "2019-10-29T18:03:19.366992: step 252, loss 0.584943, acc 0.695312\n",
            "2019-10-29T18:03:20.099524: step 253, loss 0.62813, acc 0.601562\n",
            "2019-10-29T18:03:20.839840: step 254, loss 0.684394, acc 0.59375\n",
            "2019-10-29T18:03:21.570300: step 255, loss 0.645145, acc 0.585938\n",
            "2019-10-29T18:03:22.298627: step 256, loss 0.606653, acc 0.71875\n",
            "2019-10-29T18:03:23.030596: step 257, loss 0.621615, acc 0.664062\n",
            "2019-10-29T18:03:23.758911: step 258, loss 0.614165, acc 0.632812\n",
            "2019-10-29T18:03:24.487408: step 259, loss 0.672097, acc 0.632812\n",
            "2019-10-29T18:03:25.233624: step 260, loss 0.628797, acc 0.632812\n",
            "2019-10-29T18:03:25.964008: step 261, loss 0.626972, acc 0.59375\n",
            "2019-10-29T18:03:26.702515: step 262, loss 0.642356, acc 0.625\n",
            "2019-10-29T18:03:27.439141: step 263, loss 0.660627, acc 0.59375\n",
            "2019-10-29T18:03:28.161872: step 264, loss 0.597714, acc 0.6875\n",
            "2019-10-29T18:03:28.894858: step 265, loss 0.71266, acc 0.585938\n",
            "2019-10-29T18:03:29.625566: step 266, loss 0.646279, acc 0.617188\n",
            "2019-10-29T18:03:30.371377: step 267, loss 0.635511, acc 0.640625\n",
            "2019-10-29T18:03:31.102607: step 268, loss 0.620748, acc 0.65625\n",
            "2019-10-29T18:03:31.844646: step 269, loss 0.624777, acc 0.6875\n",
            "2019-10-29T18:03:32.580632: step 270, loss 0.642359, acc 0.609375\n",
            "2019-10-29T18:03:33.324048: step 271, loss 0.650567, acc 0.617188\n",
            "2019-10-29T18:03:34.065692: step 272, loss 0.667958, acc 0.617188\n",
            "2019-10-29T18:03:34.796064: step 273, loss 0.643377, acc 0.632812\n",
            "2019-10-29T18:03:35.531659: step 274, loss 0.621131, acc 0.671875\n",
            "2019-10-29T18:03:36.271970: step 275, loss 0.609496, acc 0.632812\n",
            "2019-10-29T18:03:36.991897: step 276, loss 0.622251, acc 0.632812\n",
            "2019-10-29T18:03:37.738335: step 277, loss 0.640428, acc 0.601562\n",
            "2019-10-29T18:03:38.462562: step 278, loss 0.642232, acc 0.59375\n",
            "2019-10-29T18:03:39.187021: step 279, loss 0.661783, acc 0.59375\n",
            "2019-10-29T18:03:39.915506: step 280, loss 0.610471, acc 0.664062\n",
            "2019-10-29T18:03:40.656632: step 281, loss 0.6574, acc 0.640625\n",
            "2019-10-29T18:03:41.383674: step 282, loss 0.654405, acc 0.609375\n",
            "2019-10-29T18:03:42.116891: step 283, loss 0.644177, acc 0.632812\n",
            "2019-10-29T18:03:42.710336: step 284, loss 0.628211, acc 0.625\n",
            "2019-10-29T18:03:43.457838: step 285, loss 0.582115, acc 0.75\n",
            "2019-10-29T18:03:44.195221: step 286, loss 0.639404, acc 0.625\n",
            "2019-10-29T18:03:44.929441: step 287, loss 0.608082, acc 0.664062\n",
            "2019-10-29T18:03:45.660788: step 288, loss 0.585462, acc 0.703125\n",
            "2019-10-29T18:03:46.386434: step 289, loss 0.638672, acc 0.632812\n",
            "2019-10-29T18:03:47.129427: step 290, loss 0.586775, acc 0.648438\n",
            "2019-10-29T18:03:47.865138: step 291, loss 0.597442, acc 0.695312\n",
            "2019-10-29T18:03:48.594272: step 292, loss 0.557998, acc 0.742188\n",
            "2019-10-29T18:03:49.317933: step 293, loss 0.605047, acc 0.703125\n",
            "2019-10-29T18:03:50.044849: step 294, loss 0.598002, acc 0.703125\n",
            "2019-10-29T18:03:50.778287: step 295, loss 0.593199, acc 0.695312\n",
            "2019-10-29T18:03:51.507107: step 296, loss 0.638128, acc 0.648438\n",
            "2019-10-29T18:03:52.236652: step 297, loss 0.617813, acc 0.671875\n",
            "2019-10-29T18:03:52.992300: step 298, loss 0.616841, acc 0.671875\n",
            "2019-10-29T18:03:53.723890: step 299, loss 0.638722, acc 0.640625\n",
            "2019-10-29T18:03:54.470722: step 300, loss 0.599734, acc 0.703125\n",
            "\n",
            "Evaluation:\n",
            "2019-10-29T18:03:55.000284: step 300, loss 0.668407, acc 0.594\n",
            "\n",
            "2019-10-29T18:03:55.734501: step 301, loss 0.626345, acc 0.648438\n",
            "2019-10-29T18:03:56.476053: step 302, loss 0.584375, acc 0.648438\n",
            "2019-10-29T18:03:57.210900: step 303, loss 0.628139, acc 0.632812\n",
            "2019-10-29T18:03:57.941513: step 304, loss 0.541945, acc 0.757812\n",
            "2019-10-29T18:03:58.680383: step 305, loss 0.5672, acc 0.71875\n",
            "2019-10-29T18:03:59.409923: step 306, loss 0.529436, acc 0.734375\n",
            "2019-10-29T18:04:00.133245: step 307, loss 0.567762, acc 0.6875\n",
            "2019-10-29T18:04:00.863299: step 308, loss 0.58724, acc 0.71875\n",
            "2019-10-29T18:04:01.596562: step 309, loss 0.574009, acc 0.695312\n",
            "2019-10-29T18:04:02.337635: step 310, loss 0.603663, acc 0.710938\n",
            "2019-10-29T18:04:03.080280: step 311, loss 0.570753, acc 0.703125\n",
            "2019-10-29T18:04:03.814439: step 312, loss 0.613495, acc 0.71875\n",
            "2019-10-29T18:04:04.555284: step 313, loss 0.519633, acc 0.75\n",
            "2019-10-29T18:04:05.295076: step 314, loss 0.552867, acc 0.75\n",
            "2019-10-29T18:04:06.028991: step 315, loss 0.623523, acc 0.6875\n",
            "2019-10-29T18:04:06.757023: step 316, loss 0.610472, acc 0.703125\n",
            "2019-10-29T18:04:07.500889: step 317, loss 0.54405, acc 0.75\n",
            "2019-10-29T18:04:08.243903: step 318, loss 0.618067, acc 0.695312\n",
            "2019-10-29T18:04:08.975700: step 319, loss 0.568121, acc 0.703125\n",
            "2019-10-29T18:04:09.710643: step 320, loss 0.636996, acc 0.617188\n",
            "2019-10-29T18:04:10.474748: step 321, loss 0.638875, acc 0.640625\n",
            "2019-10-29T18:04:11.241009: step 322, loss 0.569921, acc 0.695312\n",
            "2019-10-29T18:04:11.988030: step 323, loss 0.612897, acc 0.695312\n",
            "2019-10-29T18:04:12.743152: step 324, loss 0.528805, acc 0.75\n",
            "2019-10-29T18:04:13.487388: step 325, loss 0.526842, acc 0.734375\n",
            "2019-10-29T18:04:14.242032: step 326, loss 0.566433, acc 0.726562\n",
            "2019-10-29T18:04:14.997157: step 327, loss 0.582058, acc 0.71875\n",
            "2019-10-29T18:04:15.745062: step 328, loss 0.65513, acc 0.632812\n",
            "2019-10-29T18:04:16.475209: step 329, loss 0.586623, acc 0.695312\n",
            "2019-10-29T18:04:17.224516: step 330, loss 0.624859, acc 0.632812\n",
            "2019-10-29T18:04:17.964018: step 331, loss 0.576233, acc 0.6875\n",
            "2019-10-29T18:04:18.702151: step 332, loss 0.600393, acc 0.640625\n",
            "2019-10-29T18:04:19.429030: step 333, loss 0.628521, acc 0.648438\n",
            "2019-10-29T18:04:20.176265: step 334, loss 0.566815, acc 0.703125\n",
            "2019-10-29T18:04:20.916305: step 335, loss 0.589199, acc 0.710938\n",
            "2019-10-29T18:04:21.664204: step 336, loss 0.569719, acc 0.726562\n",
            "2019-10-29T18:04:22.399673: step 337, loss 0.57874, acc 0.695312\n",
            "2019-10-29T18:04:23.125167: step 338, loss 0.567606, acc 0.679688\n",
            "2019-10-29T18:04:23.861173: step 339, loss 0.619345, acc 0.695312\n",
            "2019-10-29T18:04:24.586593: step 340, loss 0.557218, acc 0.695312\n",
            "2019-10-29T18:04:25.321690: step 341, loss 0.574298, acc 0.695312\n",
            "2019-10-29T18:04:26.053977: step 342, loss 0.527611, acc 0.742188\n",
            "2019-10-29T18:04:26.804265: step 343, loss 0.571501, acc 0.703125\n",
            "2019-10-29T18:04:27.533172: step 344, loss 0.643376, acc 0.664062\n",
            "2019-10-29T18:04:28.262346: step 345, loss 0.556639, acc 0.710938\n",
            "2019-10-29T18:04:28.986632: step 346, loss 0.645931, acc 0.664062\n",
            "2019-10-29T18:04:29.716745: step 347, loss 0.607005, acc 0.671875\n",
            "2019-10-29T18:04:30.459256: step 348, loss 0.582192, acc 0.664062\n",
            "2019-10-29T18:04:31.194411: step 349, loss 0.573273, acc 0.742188\n",
            "2019-10-29T18:04:31.930825: step 350, loss 0.576751, acc 0.6875\n",
            "2019-10-29T18:04:32.675938: step 351, loss 0.593183, acc 0.695312\n",
            "2019-10-29T18:04:33.405762: step 352, loss 0.573579, acc 0.726562\n",
            "2019-10-29T18:04:34.139063: step 353, loss 0.585087, acc 0.664062\n",
            "2019-10-29T18:04:34.874525: step 354, loss 0.592445, acc 0.671875\n",
            "2019-10-29T18:04:35.464860: step 355, loss 0.604242, acc 0.65\n",
            "2019-10-29T18:04:36.229590: step 356, loss 0.531256, acc 0.742188\n",
            "2019-10-29T18:04:36.962101: step 357, loss 0.509369, acc 0.734375\n",
            "2019-10-29T18:04:37.706336: step 358, loss 0.533956, acc 0.71875\n",
            "2019-10-29T18:04:38.434088: step 359, loss 0.617299, acc 0.679688\n",
            "2019-10-29T18:04:39.164680: step 360, loss 0.521333, acc 0.710938\n",
            "2019-10-29T18:04:39.893561: step 361, loss 0.493081, acc 0.796875\n",
            "2019-10-29T18:04:40.613502: step 362, loss 0.497892, acc 0.773438\n",
            "2019-10-29T18:04:41.345579: step 363, loss 0.532974, acc 0.734375\n",
            "2019-10-29T18:04:42.077646: step 364, loss 0.574681, acc 0.710938\n",
            "2019-10-29T18:04:42.817657: step 365, loss 0.518391, acc 0.734375\n",
            "2019-10-29T18:04:43.548990: step 366, loss 0.572472, acc 0.726562\n",
            "2019-10-29T18:04:44.277739: step 367, loss 0.594665, acc 0.632812\n",
            "2019-10-29T18:04:45.000357: step 368, loss 0.523688, acc 0.75\n",
            "2019-10-29T18:04:45.731619: step 369, loss 0.531116, acc 0.710938\n",
            "2019-10-29T18:04:46.456809: step 370, loss 0.439622, acc 0.8125\n",
            "2019-10-29T18:04:47.195888: step 371, loss 0.51893, acc 0.765625\n",
            "2019-10-29T18:04:47.932869: step 372, loss 0.465586, acc 0.773438\n",
            "2019-10-29T18:04:48.663594: step 373, loss 0.520705, acc 0.734375\n",
            "2019-10-29T18:04:49.384757: step 374, loss 0.482961, acc 0.773438\n",
            "2019-10-29T18:04:50.124291: step 375, loss 0.5178, acc 0.742188\n",
            "2019-10-29T18:04:50.858917: step 376, loss 0.470353, acc 0.75\n",
            "2019-10-29T18:04:51.587788: step 377, loss 0.559964, acc 0.679688\n",
            "2019-10-29T18:04:52.327046: step 378, loss 0.521679, acc 0.726562\n",
            "2019-10-29T18:04:53.065055: step 379, loss 0.577632, acc 0.6875\n",
            "2019-10-29T18:04:53.791457: step 380, loss 0.563806, acc 0.757812\n",
            "2019-10-29T18:04:54.529189: step 381, loss 0.549861, acc 0.726562\n",
            "2019-10-29T18:04:55.253984: step 382, loss 0.583706, acc 0.6875\n",
            "2019-10-29T18:04:56.002550: step 383, loss 0.513251, acc 0.773438\n",
            "2019-10-29T18:04:56.728024: step 384, loss 0.503728, acc 0.757812\n",
            "2019-10-29T18:04:57.456049: step 385, loss 0.515414, acc 0.726562\n",
            "2019-10-29T18:04:58.187715: step 386, loss 0.518972, acc 0.765625\n",
            "2019-10-29T18:04:58.920679: step 387, loss 0.549605, acc 0.6875\n",
            "2019-10-29T18:04:59.649884: step 388, loss 0.5377, acc 0.765625\n",
            "2019-10-29T18:05:00.389880: step 389, loss 0.497728, acc 0.765625\n",
            "2019-10-29T18:05:01.120309: step 390, loss 0.542234, acc 0.75\n",
            "2019-10-29T18:05:01.852710: step 391, loss 0.66316, acc 0.664062\n",
            "2019-10-29T18:05:02.575429: step 392, loss 0.427605, acc 0.8125\n",
            "2019-10-29T18:05:03.313786: step 393, loss 0.568658, acc 0.6875\n",
            "2019-10-29T18:05:04.052180: step 394, loss 0.526334, acc 0.734375\n",
            "2019-10-29T18:05:04.795254: step 395, loss 0.523832, acc 0.75\n",
            "2019-10-29T18:05:05.538279: step 396, loss 0.492588, acc 0.757812\n",
            "2019-10-29T18:05:06.269312: step 397, loss 0.566436, acc 0.71875\n",
            "2019-10-29T18:05:06.998814: step 398, loss 0.578871, acc 0.695312\n",
            "2019-10-29T18:05:07.731591: step 399, loss 0.488006, acc 0.773438\n",
            "2019-10-29T18:05:08.473686: step 400, loss 0.496057, acc 0.757812\n",
            "\n",
            "Evaluation:\n",
            "2019-10-29T18:05:09.013221: step 400, loss 0.647182, acc 0.632\n",
            "\n",
            "2019-10-29T18:05:09.745309: step 401, loss 0.50558, acc 0.75\n",
            "2019-10-29T18:05:10.500699: step 402, loss 0.529546, acc 0.734375\n",
            "2019-10-29T18:05:11.232667: step 403, loss 0.545149, acc 0.734375\n",
            "2019-10-29T18:05:11.965429: step 404, loss 0.485999, acc 0.765625\n",
            "2019-10-29T18:05:12.686684: step 405, loss 0.426636, acc 0.84375\n",
            "2019-10-29T18:05:13.429080: step 406, loss 0.585906, acc 0.6875\n",
            "2019-10-29T18:05:14.156735: step 407, loss 0.538935, acc 0.765625\n",
            "2019-10-29T18:05:14.889731: step 408, loss 0.678561, acc 0.65625\n",
            "2019-10-29T18:05:15.616176: step 409, loss 0.499626, acc 0.773438\n",
            "2019-10-29T18:05:16.352890: step 410, loss 0.521712, acc 0.757812\n",
            "2019-10-29T18:05:17.105137: step 411, loss 0.494918, acc 0.765625\n",
            "2019-10-29T18:05:17.840837: step 412, loss 0.499549, acc 0.78125\n",
            "2019-10-29T18:05:18.586896: step 413, loss 0.47729, acc 0.828125\n",
            "2019-10-29T18:05:19.311268: step 414, loss 0.483694, acc 0.828125\n",
            "2019-10-29T18:05:20.044125: step 415, loss 0.530678, acc 0.710938\n",
            "2019-10-29T18:05:20.788554: step 416, loss 0.630316, acc 0.703125\n",
            "2019-10-29T18:05:21.535951: step 417, loss 0.581024, acc 0.6875\n",
            "2019-10-29T18:05:22.269140: step 418, loss 0.492887, acc 0.734375\n",
            "2019-10-29T18:05:23.008593: step 419, loss 0.517335, acc 0.773438\n",
            "2019-10-29T18:05:23.737806: step 420, loss 0.507608, acc 0.757812\n",
            "2019-10-29T18:05:24.465981: step 421, loss 0.506831, acc 0.796875\n",
            "2019-10-29T18:05:25.205516: step 422, loss 0.442586, acc 0.835938\n",
            "2019-10-29T18:05:25.946665: step 423, loss 0.482946, acc 0.773438\n",
            "2019-10-29T18:05:26.685520: step 424, loss 0.477929, acc 0.75\n",
            "2019-10-29T18:05:27.443205: step 425, loss 0.542953, acc 0.726562\n",
            "2019-10-29T18:05:28.032942: step 426, loss 0.480688, acc 0.75\n",
            "2019-10-29T18:05:28.788431: step 427, loss 0.503221, acc 0.734375\n",
            "2019-10-29T18:05:29.534724: step 428, loss 0.501709, acc 0.78125\n",
            "2019-10-29T18:05:30.267717: step 429, loss 0.483773, acc 0.757812\n",
            "2019-10-29T18:05:30.993883: step 430, loss 0.481148, acc 0.773438\n",
            "2019-10-29T18:05:31.719785: step 431, loss 0.462963, acc 0.78125\n",
            "2019-10-29T18:05:32.438967: step 432, loss 0.469311, acc 0.820312\n",
            "2019-10-29T18:05:33.160804: step 433, loss 0.385456, acc 0.851562\n",
            "2019-10-29T18:05:33.892798: step 434, loss 0.407774, acc 0.789062\n",
            "2019-10-29T18:05:34.633442: step 435, loss 0.55965, acc 0.679688\n",
            "2019-10-29T18:05:35.365673: step 436, loss 0.457795, acc 0.789062\n",
            "2019-10-29T18:05:36.100053: step 437, loss 0.370573, acc 0.867188\n",
            "2019-10-29T18:05:36.848412: step 438, loss 0.491998, acc 0.765625\n",
            "2019-10-29T18:05:37.587834: step 439, loss 0.426292, acc 0.8125\n",
            "2019-10-29T18:05:38.360080: step 440, loss 0.418871, acc 0.789062\n",
            "2019-10-29T18:05:39.093256: step 441, loss 0.509913, acc 0.734375\n",
            "2019-10-29T18:05:39.811888: step 442, loss 0.456265, acc 0.78125\n",
            "2019-10-29T18:05:40.553318: step 443, loss 0.428776, acc 0.835938\n",
            "2019-10-29T18:05:41.272768: step 444, loss 0.48285, acc 0.789062\n",
            "2019-10-29T18:05:42.003125: step 445, loss 0.386015, acc 0.835938\n",
            "2019-10-29T18:05:42.734438: step 446, loss 0.415707, acc 0.804688\n",
            "2019-10-29T18:05:43.465213: step 447, loss 0.520322, acc 0.8125\n",
            "2019-10-29T18:05:44.201691: step 448, loss 0.403336, acc 0.804688\n",
            "2019-10-29T18:05:44.931026: step 449, loss 0.4742, acc 0.757812\n",
            "2019-10-29T18:05:45.672760: step 450, loss 0.483683, acc 0.820312\n",
            "2019-10-29T18:05:46.404213: step 451, loss 0.500403, acc 0.804688\n",
            "2019-10-29T18:05:47.140859: step 452, loss 0.391862, acc 0.859375\n",
            "2019-10-29T18:05:47.868065: step 453, loss 0.431812, acc 0.789062\n",
            "2019-10-29T18:05:48.604958: step 454, loss 0.405391, acc 0.859375\n",
            "2019-10-29T18:05:49.366257: step 455, loss 0.464293, acc 0.804688\n",
            "2019-10-29T18:05:50.114364: step 456, loss 0.446757, acc 0.796875\n",
            "2019-10-29T18:05:50.843902: step 457, loss 0.486073, acc 0.78125\n",
            "2019-10-29T18:05:51.571596: step 458, loss 0.464414, acc 0.75\n",
            "2019-10-29T18:05:52.301279: step 459, loss 0.430432, acc 0.804688\n",
            "2019-10-29T18:05:53.039840: step 460, loss 0.446945, acc 0.8125\n",
            "2019-10-29T18:05:53.778994: step 461, loss 0.461695, acc 0.78125\n",
            "2019-10-29T18:05:54.514569: step 462, loss 0.477935, acc 0.78125\n",
            "2019-10-29T18:05:55.246381: step 463, loss 0.435174, acc 0.789062\n",
            "2019-10-29T18:05:55.991767: step 464, loss 0.56996, acc 0.734375\n",
            "2019-10-29T18:05:56.731192: step 465, loss 0.384944, acc 0.84375\n",
            "2019-10-29T18:05:57.459695: step 466, loss 0.395085, acc 0.8125\n",
            "2019-10-29T18:05:58.196056: step 467, loss 0.457491, acc 0.742188\n",
            "2019-10-29T18:05:58.940608: step 468, loss 0.447116, acc 0.796875\n",
            "2019-10-29T18:05:59.682088: step 469, loss 0.382805, acc 0.835938\n",
            "2019-10-29T18:06:00.415660: step 470, loss 0.434714, acc 0.8125\n",
            "2019-10-29T18:06:01.151834: step 471, loss 0.401024, acc 0.828125\n",
            "2019-10-29T18:06:01.901017: step 472, loss 0.357098, acc 0.828125\n",
            "2019-10-29T18:06:02.657699: step 473, loss 0.437914, acc 0.789062\n",
            "2019-10-29T18:06:03.400960: step 474, loss 0.462352, acc 0.8125\n",
            "2019-10-29T18:06:04.141990: step 475, loss 0.408038, acc 0.8125\n",
            "2019-10-29T18:06:04.868444: step 476, loss 0.428852, acc 0.835938\n",
            "2019-10-29T18:06:05.595864: step 477, loss 0.459161, acc 0.78125\n",
            "2019-10-29T18:06:06.344874: step 478, loss 0.549997, acc 0.703125\n",
            "2019-10-29T18:06:07.077931: step 479, loss 0.425323, acc 0.8125\n",
            "2019-10-29T18:06:07.812613: step 480, loss 0.498953, acc 0.75\n",
            "2019-10-29T18:06:08.566532: step 481, loss 0.468094, acc 0.75\n",
            "2019-10-29T18:06:09.308816: step 482, loss 0.525613, acc 0.726562\n",
            "2019-10-29T18:06:10.064061: step 483, loss 0.430672, acc 0.828125\n",
            "2019-10-29T18:06:10.809227: step 484, loss 0.468896, acc 0.796875\n",
            "2019-10-29T18:06:11.539576: step 485, loss 0.564492, acc 0.6875\n",
            "2019-10-29T18:06:12.269552: step 486, loss 0.45772, acc 0.804688\n",
            "2019-10-29T18:06:12.996144: step 487, loss 0.373381, acc 0.859375\n",
            "2019-10-29T18:06:13.733021: step 488, loss 0.557293, acc 0.757812\n",
            "2019-10-29T18:06:14.472080: step 489, loss 0.388785, acc 0.851562\n",
            "2019-10-29T18:06:15.198056: step 490, loss 0.376806, acc 0.828125\n",
            "2019-10-29T18:06:15.935105: step 491, loss 0.434857, acc 0.796875\n",
            "2019-10-29T18:06:16.676198: step 492, loss 0.367703, acc 0.84375\n",
            "2019-10-29T18:06:17.403341: step 493, loss 0.499965, acc 0.742188\n",
            "2019-10-29T18:06:18.140260: step 494, loss 0.51029, acc 0.796875\n",
            "2019-10-29T18:06:18.864898: step 495, loss 0.41619, acc 0.820312\n",
            "2019-10-29T18:06:19.594028: step 496, loss 0.429982, acc 0.796875\n",
            "2019-10-29T18:06:20.178872: step 497, loss 0.5487, acc 0.75\n",
            "2019-10-29T18:06:20.978764: step 498, loss 0.323201, acc 0.867188\n",
            "2019-10-29T18:06:21.709647: step 499, loss 0.354597, acc 0.84375\n",
            "2019-10-29T18:06:22.451506: step 500, loss 0.404471, acc 0.820312\n",
            "\n",
            "Evaluation:\n",
            "2019-10-29T18:06:22.924618: step 500, loss 0.704521, acc 0.682\n",
            "\n",
            "2019-10-29T18:06:23.651984: step 501, loss 0.457058, acc 0.8125\n",
            "2019-10-29T18:06:24.382438: step 502, loss 0.429175, acc 0.820312\n",
            "2019-10-29T18:06:25.120771: step 503, loss 0.46225, acc 0.765625\n",
            "2019-10-29T18:06:25.850851: step 504, loss 0.419772, acc 0.789062\n",
            "2019-10-29T18:06:26.584067: step 505, loss 0.358768, acc 0.875\n",
            "2019-10-29T18:06:27.311385: step 506, loss 0.40093, acc 0.796875\n",
            "2019-10-29T18:06:28.057246: step 507, loss 0.370332, acc 0.851562\n",
            "2019-10-29T18:06:28.793577: step 508, loss 0.352937, acc 0.867188\n",
            "2019-10-29T18:06:29.532616: step 509, loss 0.378586, acc 0.835938\n",
            "2019-10-29T18:06:30.273341: step 510, loss 0.303699, acc 0.875\n",
            "2019-10-29T18:06:31.020653: step 511, loss 0.436579, acc 0.8125\n",
            "2019-10-29T18:06:31.753723: step 512, loss 0.37983, acc 0.851562\n",
            "2019-10-29T18:06:32.501254: step 513, loss 0.334352, acc 0.851562\n",
            "2019-10-29T18:06:33.244222: step 514, loss 0.312052, acc 0.875\n",
            "2019-10-29T18:06:33.993047: step 515, loss 0.349832, acc 0.835938\n",
            "2019-10-29T18:06:34.736936: step 516, loss 0.368873, acc 0.828125\n",
            "2019-10-29T18:06:35.472452: step 517, loss 0.440836, acc 0.78125\n",
            "2019-10-29T18:06:36.212590: step 518, loss 0.4079, acc 0.8125\n",
            "2019-10-29T18:06:36.948770: step 519, loss 0.468834, acc 0.773438\n",
            "2019-10-29T18:06:37.685688: step 520, loss 0.442218, acc 0.796875\n",
            "2019-10-29T18:06:38.450450: step 521, loss 0.452864, acc 0.796875\n",
            "2019-10-29T18:06:39.184150: step 522, loss 0.43695, acc 0.78125\n",
            "2019-10-29T18:06:39.930356: step 523, loss 0.360334, acc 0.835938\n",
            "2019-10-29T18:06:40.667699: step 524, loss 0.357698, acc 0.84375\n",
            "2019-10-29T18:06:41.409863: step 525, loss 0.32464, acc 0.835938\n",
            "2019-10-29T18:06:42.159119: step 526, loss 0.449955, acc 0.78125\n",
            "2019-10-29T18:06:42.895167: step 527, loss 0.423232, acc 0.820312\n",
            "2019-10-29T18:06:43.631523: step 528, loss 0.386659, acc 0.867188\n",
            "2019-10-29T18:06:44.359402: step 529, loss 0.361636, acc 0.84375\n",
            "2019-10-29T18:06:45.087645: step 530, loss 0.35784, acc 0.828125\n",
            "2019-10-29T18:06:45.816520: step 531, loss 0.398079, acc 0.820312\n",
            "2019-10-29T18:06:46.554012: step 532, loss 0.317516, acc 0.859375\n",
            "2019-10-29T18:06:47.291353: step 533, loss 0.423634, acc 0.8125\n",
            "2019-10-29T18:06:48.025210: step 534, loss 0.33139, acc 0.867188\n",
            "2019-10-29T18:06:48.762594: step 535, loss 0.336606, acc 0.851562\n",
            "2019-10-29T18:06:49.501783: step 536, loss 0.323378, acc 0.890625\n",
            "2019-10-29T18:06:50.238407: step 537, loss 0.270465, acc 0.867188\n",
            "2019-10-29T18:06:50.966241: step 538, loss 0.419771, acc 0.84375\n",
            "2019-10-29T18:06:51.713987: step 539, loss 0.305597, acc 0.867188\n",
            "2019-10-29T18:06:52.470587: step 540, loss 0.382546, acc 0.8125\n",
            "2019-10-29T18:06:53.198988: step 541, loss 0.402026, acc 0.78125\n",
            "2019-10-29T18:06:53.931888: step 542, loss 0.403676, acc 0.835938\n",
            "2019-10-29T18:06:54.676651: step 543, loss 0.430604, acc 0.820312\n",
            "2019-10-29T18:06:55.401599: step 544, loss 0.436686, acc 0.78125\n",
            "2019-10-29T18:06:56.131074: step 545, loss 0.39687, acc 0.820312\n",
            "2019-10-29T18:06:56.868048: step 546, loss 0.451079, acc 0.804688\n",
            "2019-10-29T18:06:57.595120: step 547, loss 0.473825, acc 0.820312\n",
            "2019-10-29T18:06:58.334831: step 548, loss 0.427427, acc 0.773438\n",
            "2019-10-29T18:06:59.086922: step 549, loss 0.405907, acc 0.8125\n",
            "2019-10-29T18:06:59.826427: step 550, loss 0.417685, acc 0.8125\n",
            "2019-10-29T18:07:00.567408: step 551, loss 0.442183, acc 0.828125\n",
            "2019-10-29T18:07:01.304209: step 552, loss 0.313148, acc 0.867188\n",
            "2019-10-29T18:07:02.037017: step 553, loss 0.392524, acc 0.820312\n",
            "2019-10-29T18:07:02.777847: step 554, loss 0.406168, acc 0.851562\n",
            "2019-10-29T18:07:03.520748: step 555, loss 0.338867, acc 0.867188\n",
            "2019-10-29T18:07:04.266010: step 556, loss 0.372337, acc 0.84375\n",
            "2019-10-29T18:07:04.999401: step 557, loss 0.315728, acc 0.84375\n",
            "2019-10-29T18:07:05.735413: step 558, loss 0.342159, acc 0.835938\n",
            "2019-10-29T18:07:06.475614: step 559, loss 0.388503, acc 0.8125\n",
            "2019-10-29T18:07:07.211914: step 560, loss 0.374668, acc 0.820312\n",
            "2019-10-29T18:07:07.944654: step 561, loss 0.348459, acc 0.859375\n",
            "2019-10-29T18:07:08.696733: step 562, loss 0.46201, acc 0.789062\n",
            "2019-10-29T18:07:09.427566: step 563, loss 0.345523, acc 0.84375\n",
            "2019-10-29T18:07:10.166769: step 564, loss 0.289031, acc 0.875\n",
            "2019-10-29T18:07:10.900770: step 565, loss 0.44753, acc 0.804688\n",
            "2019-10-29T18:07:11.633133: step 566, loss 0.337469, acc 0.867188\n",
            "2019-10-29T18:07:12.369548: step 567, loss 0.429157, acc 0.796875\n",
            "2019-10-29T18:07:12.958588: step 568, loss 0.299052, acc 0.9\n",
            "2019-10-29T18:07:13.731336: step 569, loss 0.313034, acc 0.867188\n",
            "2019-10-29T18:07:14.475630: step 570, loss 0.38173, acc 0.828125\n",
            "2019-10-29T18:07:15.207394: step 571, loss 0.368502, acc 0.828125\n",
            "2019-10-29T18:07:15.945284: step 572, loss 0.328247, acc 0.890625\n",
            "2019-10-29T18:07:16.672900: step 573, loss 0.295417, acc 0.890625\n",
            "2019-10-29T18:07:17.399959: step 574, loss 0.353198, acc 0.859375\n",
            "2019-10-29T18:07:18.131032: step 575, loss 0.336669, acc 0.875\n",
            "2019-10-29T18:07:18.869727: step 576, loss 0.272338, acc 0.898438\n",
            "2019-10-29T18:07:19.598457: step 577, loss 0.330973, acc 0.84375\n",
            "2019-10-29T18:07:20.329600: step 578, loss 0.358818, acc 0.84375\n",
            "2019-10-29T18:07:21.057747: step 579, loss 0.26346, acc 0.890625\n",
            "2019-10-29T18:07:21.784748: step 580, loss 0.323068, acc 0.859375\n",
            "2019-10-29T18:07:22.505977: step 581, loss 0.436148, acc 0.804688\n",
            "2019-10-29T18:07:23.236327: step 582, loss 0.382914, acc 0.835938\n",
            "2019-10-29T18:07:24.039663: step 583, loss 0.284607, acc 0.890625\n",
            "2019-10-29T18:07:25.029382: step 584, loss 0.34741, acc 0.859375\n",
            "2019-10-29T18:07:25.756987: step 585, loss 0.410216, acc 0.820312\n",
            "2019-10-29T18:07:26.483887: step 586, loss 0.312167, acc 0.867188\n",
            "2019-10-29T18:07:27.218840: step 587, loss 0.342508, acc 0.851562\n",
            "2019-10-29T18:07:27.951200: step 588, loss 0.340675, acc 0.859375\n",
            "2019-10-29T18:07:28.670249: step 589, loss 0.303658, acc 0.875\n",
            "2019-10-29T18:07:29.405990: step 590, loss 0.297563, acc 0.875\n",
            "2019-10-29T18:07:30.134339: step 591, loss 0.370793, acc 0.84375\n",
            "2019-10-29T18:07:30.872811: step 592, loss 0.245483, acc 0.867188\n",
            "2019-10-29T18:07:31.610959: step 593, loss 0.244431, acc 0.898438\n",
            "2019-10-29T18:07:32.348506: step 594, loss 0.303166, acc 0.851562\n",
            "2019-10-29T18:07:33.087328: step 595, loss 0.218068, acc 0.921875\n",
            "2019-10-29T18:07:33.824619: step 596, loss 0.28782, acc 0.898438\n",
            "2019-10-29T18:07:34.576671: step 597, loss 0.367808, acc 0.859375\n",
            "2019-10-29T18:07:35.321156: step 598, loss 0.319248, acc 0.867188\n",
            "2019-10-29T18:07:36.056746: step 599, loss 0.382594, acc 0.835938\n",
            "2019-10-29T18:07:36.799330: step 600, loss 0.339404, acc 0.875\n",
            "\n",
            "Evaluation:\n",
            "2019-10-29T18:07:37.435919: step 600, loss 0.681361, acc 0.67\n",
            "\n",
            "2019-10-29T18:07:38.166855: step 601, loss 0.359672, acc 0.828125\n",
            "2019-10-29T18:07:38.935836: step 602, loss 0.355735, acc 0.859375\n",
            "2019-10-29T18:07:39.697420: step 603, loss 0.412445, acc 0.804688\n",
            "2019-10-29T18:07:40.469306: step 604, loss 0.414231, acc 0.828125\n",
            "2019-10-29T18:07:41.224767: step 605, loss 0.366364, acc 0.867188\n",
            "2019-10-29T18:07:41.984750: step 606, loss 0.419282, acc 0.796875\n",
            "2019-10-29T18:07:42.737417: step 607, loss 0.398831, acc 0.8125\n",
            "2019-10-29T18:07:43.500380: step 608, loss 0.35251, acc 0.851562\n",
            "2019-10-29T18:07:44.267150: step 609, loss 0.327191, acc 0.859375\n",
            "2019-10-29T18:07:45.028220: step 610, loss 0.289778, acc 0.898438\n",
            "2019-10-29T18:07:45.797707: step 611, loss 0.260378, acc 0.898438\n",
            "2019-10-29T18:07:46.555844: step 612, loss 0.366734, acc 0.84375\n",
            "2019-10-29T18:07:47.302260: step 613, loss 0.3505, acc 0.867188\n",
            "2019-10-29T18:07:48.063406: step 614, loss 0.367611, acc 0.859375\n",
            "2019-10-29T18:07:48.818347: step 615, loss 0.326973, acc 0.875\n",
            "2019-10-29T18:07:49.565753: step 616, loss 0.274952, acc 0.875\n",
            "2019-10-29T18:07:50.295192: step 617, loss 0.23885, acc 0.90625\n",
            "2019-10-29T18:07:51.032152: step 618, loss 0.399623, acc 0.8125\n",
            "2019-10-29T18:07:51.769385: step 619, loss 0.392121, acc 0.820312\n",
            "2019-10-29T18:07:52.508706: step 620, loss 0.351447, acc 0.835938\n",
            "2019-10-29T18:07:53.239508: step 621, loss 0.348711, acc 0.835938\n",
            "2019-10-29T18:07:53.974762: step 622, loss 0.304649, acc 0.890625\n",
            "2019-10-29T18:07:54.707369: step 623, loss 0.31104, acc 0.898438\n",
            "2019-10-29T18:07:55.441800: step 624, loss 0.34606, acc 0.867188\n",
            "2019-10-29T18:07:56.184836: step 625, loss 0.284612, acc 0.882812\n",
            "2019-10-29T18:07:56.936857: step 626, loss 0.30804, acc 0.859375\n",
            "2019-10-29T18:07:57.663662: step 627, loss 0.266665, acc 0.898438\n",
            "2019-10-29T18:07:58.394949: step 628, loss 0.357106, acc 0.851562\n",
            "2019-10-29T18:07:59.128848: step 629, loss 0.361672, acc 0.835938\n",
            "2019-10-29T18:07:59.853158: step 630, loss 0.356037, acc 0.851562\n",
            "2019-10-29T18:08:00.597759: step 631, loss 0.390673, acc 0.820312\n",
            "2019-10-29T18:08:01.328986: step 632, loss 0.36124, acc 0.84375\n",
            "2019-10-29T18:08:02.061109: step 633, loss 0.430532, acc 0.820312\n",
            "2019-10-29T18:08:02.787275: step 634, loss 0.272889, acc 0.914062\n",
            "2019-10-29T18:08:03.529732: step 635, loss 0.305311, acc 0.890625\n",
            "2019-10-29T18:08:04.263497: step 636, loss 0.259751, acc 0.914062\n",
            "2019-10-29T18:08:04.996736: step 637, loss 0.344525, acc 0.84375\n",
            "2019-10-29T18:08:05.727652: step 638, loss 0.33648, acc 0.867188\n",
            "2019-10-29T18:08:06.317730: step 639, loss 0.304886, acc 0.875\n",
            "2019-10-29T18:08:07.117637: step 640, loss 0.336403, acc 0.851562\n",
            "2019-10-29T18:08:07.844240: step 641, loss 0.194612, acc 0.898438\n",
            "2019-10-29T18:08:08.588115: step 642, loss 0.252716, acc 0.890625\n",
            "2019-10-29T18:08:09.324029: step 643, loss 0.274777, acc 0.90625\n",
            "2019-10-29T18:08:10.038635: step 644, loss 0.445547, acc 0.875\n",
            "2019-10-29T18:08:10.770994: step 645, loss 0.320504, acc 0.84375\n",
            "2019-10-29T18:08:11.506186: step 646, loss 0.31241, acc 0.867188\n",
            "2019-10-29T18:08:12.249796: step 647, loss 0.251155, acc 0.890625\n",
            "2019-10-29T18:08:12.974207: step 648, loss 0.322583, acc 0.851562\n",
            "2019-10-29T18:08:13.708284: step 649, loss 0.291066, acc 0.867188\n",
            "2019-10-29T18:08:14.455210: step 650, loss 0.338934, acc 0.867188\n",
            "2019-10-29T18:08:15.176358: step 651, loss 0.324584, acc 0.84375\n",
            "2019-10-29T18:08:15.920092: step 652, loss 0.180502, acc 0.9375\n",
            "2019-10-29T18:08:16.655976: step 653, loss 0.307988, acc 0.867188\n",
            "2019-10-29T18:08:17.403957: step 654, loss 0.377238, acc 0.8125\n",
            "2019-10-29T18:08:18.150577: step 655, loss 0.255335, acc 0.921875\n",
            "2019-10-29T18:08:18.895199: step 656, loss 0.264193, acc 0.882812\n",
            "2019-10-29T18:08:19.642367: step 657, loss 0.224524, acc 0.890625\n",
            "2019-10-29T18:08:20.371775: step 658, loss 0.302893, acc 0.875\n",
            "2019-10-29T18:08:21.106058: step 659, loss 0.185068, acc 0.921875\n",
            "2019-10-29T18:08:21.843052: step 660, loss 0.232659, acc 0.929688\n",
            "2019-10-29T18:08:22.574342: step 661, loss 0.224483, acc 0.929688\n",
            "2019-10-29T18:08:23.308321: step 662, loss 0.397788, acc 0.84375\n",
            "2019-10-29T18:08:24.043159: step 663, loss 0.331129, acc 0.828125\n",
            "2019-10-29T18:08:24.776713: step 664, loss 0.311955, acc 0.882812\n",
            "2019-10-29T18:08:25.505386: step 665, loss 0.267695, acc 0.898438\n",
            "2019-10-29T18:08:26.242933: step 666, loss 0.251266, acc 0.890625\n",
            "2019-10-29T18:08:26.981585: step 667, loss 0.264829, acc 0.890625\n",
            "2019-10-29T18:08:27.724166: step 668, loss 0.411634, acc 0.804688\n",
            "2019-10-29T18:08:28.492402: step 669, loss 0.396299, acc 0.867188\n",
            "2019-10-29T18:08:29.228288: step 670, loss 0.308456, acc 0.882812\n",
            "2019-10-29T18:08:29.975669: step 671, loss 0.334006, acc 0.851562\n",
            "2019-10-29T18:08:30.714259: step 672, loss 0.205953, acc 0.945312\n",
            "2019-10-29T18:08:31.449688: step 673, loss 0.291145, acc 0.867188\n",
            "2019-10-29T18:08:32.192604: step 674, loss 0.302032, acc 0.867188\n",
            "2019-10-29T18:08:32.933354: step 675, loss 0.264208, acc 0.914062\n",
            "2019-10-29T18:08:33.664373: step 676, loss 0.350693, acc 0.867188\n",
            "2019-10-29T18:08:34.397123: step 677, loss 0.436669, acc 0.789062\n",
            "2019-10-29T18:08:35.128385: step 678, loss 0.263861, acc 0.898438\n",
            "2019-10-29T18:08:35.865689: step 679, loss 0.295976, acc 0.890625\n",
            "2019-10-29T18:08:36.602917: step 680, loss 0.354751, acc 0.851562\n",
            "2019-10-29T18:08:37.351530: step 681, loss 0.303163, acc 0.882812\n",
            "2019-10-29T18:08:38.092807: step 682, loss 0.315908, acc 0.882812\n",
            "2019-10-29T18:08:38.859970: step 683, loss 0.271001, acc 0.90625\n",
            "2019-10-29T18:08:39.603031: step 684, loss 0.272032, acc 0.914062\n",
            "2019-10-29T18:08:40.343976: step 685, loss 0.286971, acc 0.890625\n",
            "2019-10-29T18:08:41.095395: step 686, loss 0.247117, acc 0.890625\n",
            "2019-10-29T18:08:41.849438: step 687, loss 0.31264, acc 0.882812\n",
            "2019-10-29T18:08:42.589580: step 688, loss 0.227116, acc 0.921875\n",
            "2019-10-29T18:08:43.333759: step 689, loss 0.29745, acc 0.851562\n",
            "2019-10-29T18:08:44.070369: step 690, loss 0.338896, acc 0.882812\n",
            "2019-10-29T18:08:44.814122: step 691, loss 0.295668, acc 0.851562\n",
            "2019-10-29T18:08:45.553640: step 692, loss 0.23951, acc 0.882812\n",
            "2019-10-29T18:08:46.295377: step 693, loss 0.294829, acc 0.84375\n",
            "2019-10-29T18:08:47.040231: step 694, loss 0.260592, acc 0.898438\n",
            "2019-10-29T18:08:47.784345: step 695, loss 0.341263, acc 0.851562\n",
            "2019-10-29T18:08:48.516828: step 696, loss 0.343773, acc 0.84375\n",
            "2019-10-29T18:08:49.277516: step 697, loss 0.239337, acc 0.90625\n",
            "2019-10-29T18:08:50.011662: step 698, loss 0.251691, acc 0.882812\n",
            "2019-10-29T18:08:50.743685: step 699, loss 0.27177, acc 0.875\n",
            "2019-10-29T18:08:51.474299: step 700, loss 0.309467, acc 0.851562\n",
            "\n",
            "Evaluation:\n",
            "2019-10-29T18:08:52.044829: step 700, loss 0.80802, acc 0.669\n",
            "\n",
            "2019-10-29T18:08:52.773862: step 701, loss 0.269573, acc 0.914062\n",
            "2019-10-29T18:08:53.505523: step 702, loss 0.35647, acc 0.828125\n",
            "2019-10-29T18:08:54.250664: step 703, loss 0.33974, acc 0.828125\n",
            "2019-10-29T18:08:54.988910: step 704, loss 0.239972, acc 0.898438\n",
            "2019-10-29T18:08:55.747054: step 705, loss 0.296124, acc 0.882812\n",
            "2019-10-29T18:08:56.489667: step 706, loss 0.391573, acc 0.835938\n",
            "2019-10-29T18:08:57.228054: step 707, loss 0.28634, acc 0.867188\n",
            "2019-10-29T18:08:57.962898: step 708, loss 0.328883, acc 0.859375\n",
            "2019-10-29T18:08:58.705664: step 709, loss 0.342034, acc 0.859375\n",
            "2019-10-29T18:08:59.282087: step 710, loss 0.260528, acc 0.9\n",
            "2019-10-29T18:08:59.875069: step 710, loss 0.714706, acc 0.687\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jy_oq7Y_wOcV",
        "colab_type": "text"
      },
      "source": [
        "**Training using other parameters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WhxFZX0uNHP",
        "colab_type": "code",
        "outputId": "a4688311-79fb-445c-dad5-bc20a29cb90c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#! /usr/bin/env python\n",
        "import sys\n",
        "\n",
        "#SELECT WHICH MODEL YOU WISH TO RUN:\n",
        "from cnn_lstm import CNN_LSTM   #OPTION 0\n",
        "from lstm_cnn import LSTM_CNN   #OPTION 1\n",
        "from cnn import CNN             #OPTION 2 (Model by: Danny Britz)\n",
        "from lstm import LSTM           #OPTION 3\n",
        "MODEL_TO_RUN = 0\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import datetime\n",
        "import batchgen\n",
        "from tensorflow.contrib import learn\n",
        "\n",
        "from IPython import embed\n",
        "\n",
        "# Parameters\n",
        "# ==================================================\n",
        "\n",
        "# Data loading params\n",
        "dev_size = .10\n",
        "\n",
        "# Model Hyperparameters\n",
        "embedding_dim  = 64     #128   #changing the value from 32 to 64\n",
        "max_seq_legth = 70 \n",
        "filter_sizes = [3,4,5]  #3\n",
        "num_filters = 32\n",
        "dropout_prob = 0.5 #0.5\n",
        "l2_reg_lambda = 0.0\n",
        "use_glove = True #Do we use glove\n",
        "\n",
        "# Training parameters\n",
        "batch_size = 64                  # changing the batch size to 64 from 128 - I think since the data size is less, the batch size should be relevant to that\n",
        "num_epochs = 100 #200                # increasing the number of training epochs to 100 from 10\n",
        "evaluate_every = 100 #100\n",
        "checkpoint_every = 100000 #100\n",
        "num_checkpoints = 0 #Checkpoints to store\n",
        "\n",
        "\n",
        "# Misc Parameters\n",
        "allow_soft_placement = True\n",
        "log_device_placement = False\n",
        "\n",
        "\n",
        "\n",
        "# Data Preparation\n",
        "# ==================================================\n",
        "\n",
        "\n",
        "filename = \"Sentiment Analysis Dataset.csv\"\n",
        "goodfile = \"good_small.txt\"\n",
        "badfile = \"bad_small.txt\"\n",
        "\n",
        "\n",
        "# Load data\n",
        "print(\"Loading data...\")\n",
        "x_text, y = batchgen.get_dataset(goodfile, badfile, 5000) #TODO: MAX LENGTH\n",
        "\n",
        "# Build vocabulary\n",
        "max_document_length = max([len(x.split(\" \")) for x in x_text])\n",
        "if(not use_glove):\n",
        "    print (\"Not using GloVe\")\n",
        "    vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
        "    x = np.array(list(vocab_processor.fit_transform(x_text)))\n",
        "else:\n",
        "    print (\"Using GloVe\")\n",
        "    embedding_dim = 50\n",
        "    filename = './glove.6B.50d.txt'\n",
        "    def loadGloVe(filename):\n",
        "        vocab = []\n",
        "        embd = []\n",
        "        file = open(filename,'r')\n",
        "        for line in file.readlines():\n",
        "            row = line.strip().split(' ')\n",
        "            vocab.append(row[0])\n",
        "            embd.append(row[1:])\n",
        "        print('Loaded GloVe!')\n",
        "        file.close()\n",
        "        return vocab,embd\n",
        "    vocab,embd = loadGloVe(filename)\n",
        "    vocab_size = len(vocab)\n",
        "    embedding_dim = len(embd[0])\n",
        "    embedding = np.asarray(embd)\n",
        "\n",
        "    W = tf.Variable(tf.constant(0.0, shape=[vocab_size, embedding_dim]),\n",
        "                    trainable=False, name=\"W\")\n",
        "    embedding_placeholder = tf.placeholder(tf.float32, [vocab_size, embedding_dim])\n",
        "    embedding_init = W.assign(embedding_placeholder)\n",
        "\n",
        "    session_conf = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
        "    sess = tf.Session(config=session_conf)\n",
        "    sess.run(embedding_init, feed_dict={embedding_placeholder: embedding})\n",
        "\n",
        "    from tensorflow.contrib import learn\n",
        "    #init vocab processor\n",
        "    vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
        "    #fit the vocab from glove\n",
        "    pretrain = vocab_processor.fit(vocab)\n",
        "    #transform inputs\n",
        "    x = np.array(list(vocab_processor.transform(x_text)))\n",
        "\n",
        "    #init vocab processor\n",
        "    vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
        "    #fit the vocab from glove\n",
        "    pretrain = vocab_processor.fit(vocab)\n",
        "    #transform inputs\n",
        "    x = np.array(list(vocab_processor.transform(x_text)))\n",
        "\n",
        "\n",
        "# Randomly shuffle data\n",
        "np.random.seed(42)\n",
        "shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
        "x_shuffled = x[shuffle_indices]\n",
        "y_shuffled = y[shuffle_indices]\n",
        "\n",
        "# Split train/test set\n",
        "# TODO: This is very crude, should use cross-validation\n",
        "dev_sample_index = -1 * int(dev_size * float(len(y)))\n",
        "x_train, x_dev = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]\n",
        "y_train, y_dev = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]\n",
        "print(\"Vocabulary Size: {:d}\".format(len(vocab_processor.vocabulary_)))\n",
        "print(\"Train/Dev split: {:d}/{:d}\".format(len(y_train), len(y_dev)))\n",
        "\n",
        "#embed()\n",
        "\n",
        "\n",
        "# Training\n",
        "# ==================================================\n",
        "\n",
        "with tf.Graph().as_default():\n",
        "    session_conf = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
        "    sess = tf.Session(config=session_conf)\n",
        "    with sess.as_default():\n",
        "        #embed()\n",
        "        if (MODEL_TO_RUN == 0):\n",
        "            model = CNN_LSTM(x_train.shape[1],y_train.shape[1],len(vocab_processor.vocabulary_),embedding_dim,filter_sizes,num_filters,l2_reg_lambda)\n",
        "        elif (MODEL_TO_RUN == 1):\n",
        "            model = LSTM_CNN(x_train.shape[1],y_train.shape[1],len(vocab_processor.vocabulary_),embedding_dim,filter_sizes,num_filters,l2_reg_lambda)\n",
        "        elif (MODEL_TO_RUN == 2):\n",
        "            model = CNN(x_train.shape[1],y_train.shape[1],len(vocab_processor.vocabulary_),embedding_dim,filter_sizes,num_filters,l2_reg_lambda)\n",
        "        elif (MODEL_TO_RUN == 3):\n",
        "            model = LSTM(x_train.shape[1],y_train.shape[1],len(vocab_processor.vocabulary_),embedding_dim)\n",
        "        else:\n",
        "            print (\"PLEASE CHOOSE A VALID MODEL!\\n0 = CNN_LSTM\\n1 = LSTM_CNN\\n2 = CNN\\n3 = LSTM\\n\")\n",
        "            exit();\n",
        "\n",
        "\n",
        "        # Define Training procedure\n",
        "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
        "        optimizer = tf.train.AdamOptimizer(1e-3)\n",
        "        grads_and_vars = optimizer.compute_gradients(model.loss)\n",
        "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
        "\n",
        "        # Keep track of gradient values and sparsity (optional)\n",
        "        grad_summaries = []\n",
        "        for g, v in grads_and_vars:\n",
        "            if g is not None:\n",
        "                grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
        "                sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
        "                grad_summaries.append(grad_hist_summary)\n",
        "                grad_summaries.append(sparsity_summary)\n",
        "        grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
        "\n",
        "        # Output directory for models and summaries\n",
        "        timestamp = str(int(time.time()))\n",
        "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
        "        print(\"Writing to {}\\n\".format(out_dir))\n",
        "\n",
        "        # Summaries for loss and accuracy\n",
        "        loss_summary = tf.summary.scalar(\"loss\", model.loss)\n",
        "        acc_summary = tf.summary.scalar(\"accuracy\", model.accuracy)\n",
        "\n",
        "        # Train Summaries\n",
        "        train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
        "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
        "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
        "\n",
        "        # Dev summaries\n",
        "        dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
        "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
        "        dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
        "\n",
        "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
        "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
        "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
        "        if not os.path.exists(checkpoint_dir):\n",
        "            os.makedirs(checkpoint_dir)\n",
        "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=num_checkpoints)\n",
        "\n",
        "        # Write vocabulary\n",
        "        vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
        "\n",
        "        # Initialize all variables\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "\n",
        "        #TRAINING STEP\n",
        "        def train_step(x_batch, y_batch,save=False):\n",
        "            feed_dict = {\n",
        "              model.input_x: x_batch,\n",
        "              model.input_y: y_batch,\n",
        "              model.dropout_keep_prob: dropout_prob\n",
        "            }\n",
        "            _, step, summaries, loss, accuracy = sess.run(\n",
        "                [train_op, global_step, train_summary_op, model.loss, model.accuracy],\n",
        "                feed_dict)\n",
        "            time_str = datetime.datetime.now().isoformat()\n",
        "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
        "            if save:\n",
        "                train_summary_writer.add_summary(summaries, step)\n",
        "\n",
        "        #EVALUATE MODEL\n",
        "        def dev_step(x_batch, y_batch, writer=None,save=False):\n",
        "            feed_dict = {\n",
        "              model.input_x: x_batch,\n",
        "              model.input_y: y_batch,\n",
        "              model.dropout_keep_prob: 0.5\n",
        "            }\n",
        "            step, summaries, loss, accuracy = sess.run(\n",
        "                [global_step, dev_summary_op, model.loss, model.accuracy],\n",
        "                feed_dict)\n",
        "            time_str = datetime.datetime.now().isoformat()\n",
        "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
        "            if save:\n",
        "                if writer:\n",
        "                    writer.add_summary(summaries, step)\n",
        "\n",
        "        #CREATE THE BATCHES GENERATOR\n",
        "        batches = batchgen.gen_batch(list(zip(x_train, y_train)), batch_size, num_epochs)\n",
        "        \n",
        "        #TRAIN FOR EACH BATCH\n",
        "        for batch in batches:\n",
        "            x_batch, y_batch = zip(*batch)\n",
        "            train_step(x_batch, y_batch)\n",
        "            current_step = tf.train.global_step(sess, global_step)\n",
        "            if current_step % evaluate_every == 0:\n",
        "                print(\"\\nEvaluation:\")\n",
        "                dev_step(x_dev, y_dev, writer=dev_summary_writer)\n",
        "                print(\"\")\n",
        "            if current_step % checkpoint_every == 0:\n",
        "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
        "                print(\"Saved model checkpoint to {}\\n\".format(path))\n",
        "        dev_step(x_dev, y_dev, writer=dev_summary_writer)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "Using GloVe\n",
            "Loaded GloVe!\n",
            "Vocabulary Size: 370847\n",
            "Train/Dev split: 9000/1000\n",
            "(!) LOADED CNN-LSTM! :)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
            "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
            "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/hist is illegal; using conv-maxpool-3/W_0/grad/hist instead.\n",
            "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/sparsity is illegal; using conv-maxpool-3/W_0/grad/sparsity instead.\n",
            "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/hist is illegal; using conv-maxpool-3/b_0/grad/hist instead.\n",
            "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/sparsity is illegal; using conv-maxpool-3/b_0/grad/sparsity instead.\n",
            "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/hist is illegal; using conv-maxpool-4/W_0/grad/hist instead.\n",
            "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/sparsity is illegal; using conv-maxpool-4/W_0/grad/sparsity instead.\n",
            "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/hist is illegal; using conv-maxpool-4/b_0/grad/hist instead.\n",
            "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/sparsity is illegal; using conv-maxpool-4/b_0/grad/sparsity instead.\n",
            "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/hist is illegal; using conv-maxpool-5/W_0/grad/hist instead.\n",
            "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/sparsity is illegal; using conv-maxpool-5/W_0/grad/sparsity instead.\n",
            "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/hist is illegal; using conv-maxpool-5/b_0/grad/hist instead.\n",
            "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/sparsity is illegal; using conv-maxpool-5/b_0/grad/sparsity instead.\n",
            "INFO:tensorflow:Summary name rnn/lstm_cell/kernel:0/grad/hist is illegal; using rnn/lstm_cell/kernel_0/grad/hist instead.\n",
            "INFO:tensorflow:Summary name rnn/lstm_cell/kernel:0/grad/sparsity is illegal; using rnn/lstm_cell/kernel_0/grad/sparsity instead.\n",
            "INFO:tensorflow:Summary name rnn/lstm_cell/bias:0/grad/hist is illegal; using rnn/lstm_cell/bias_0/grad/hist instead.\n",
            "INFO:tensorflow:Summary name rnn/lstm_cell/bias:0/grad/sparsity is illegal; using rnn/lstm_cell/bias_0/grad/sparsity instead.\n",
            "INFO:tensorflow:Summary name Variable:0/grad/hist is illegal; using Variable_0/grad/hist instead.\n",
            "INFO:tensorflow:Summary name Variable:0/grad/sparsity is illegal; using Variable_0/grad/sparsity instead.\n",
            "INFO:tensorflow:Summary name Variable_1:0/grad/hist is illegal; using Variable_1_0/grad/hist instead.\n",
            "INFO:tensorflow:Summary name Variable_1:0/grad/sparsity is illegal; using Variable_1_0/grad/sparsity instead.\n",
            "Writing to /content/gdrive/My Drive/CS291K-master/runs/1572378849\n",
            "\n",
            "2019-10-29T19:54:12.084924: step 1, loss 0.709935, acc 0.65625\n",
            "2019-10-29T19:54:12.735101: step 2, loss 0.833395, acc 0.359375\n",
            "2019-10-29T19:54:13.350752: step 3, loss 0.688132, acc 0.515625\n",
            "2019-10-29T19:54:13.970014: step 4, loss 0.743018, acc 0.5\n",
            "2019-10-29T19:54:14.584957: step 5, loss 0.731619, acc 0.40625\n",
            "2019-10-29T19:54:15.197819: step 6, loss 0.678648, acc 0.5625\n",
            "2019-10-29T19:54:15.816514: step 7, loss 0.798889, acc 0.4375\n",
            "2019-10-29T19:54:16.465338: step 8, loss 0.71282, acc 0.40625\n",
            "2019-10-29T19:54:17.084555: step 9, loss 0.660507, acc 0.609375\n",
            "2019-10-29T19:54:17.704670: step 10, loss 0.828739, acc 0.5\n",
            "2019-10-29T19:54:18.317110: step 11, loss 0.828527, acc 0.453125\n",
            "2019-10-29T19:54:18.929170: step 12, loss 0.691857, acc 0.53125\n",
            "2019-10-29T19:54:19.554255: step 13, loss 0.678347, acc 0.546875\n",
            "2019-10-29T19:54:20.164062: step 14, loss 0.770915, acc 0.34375\n",
            "2019-10-29T19:54:20.785187: step 15, loss 0.689113, acc 0.5625\n",
            "2019-10-29T19:54:21.405848: step 16, loss 0.738355, acc 0.515625\n",
            "2019-10-29T19:54:22.019109: step 17, loss 0.74329, acc 0.4375\n",
            "2019-10-29T19:54:22.671132: step 18, loss 0.673692, acc 0.5625\n",
            "2019-10-29T19:54:23.281076: step 19, loss 0.721315, acc 0.40625\n",
            "2019-10-29T19:54:23.890957: step 20, loss 0.712483, acc 0.53125\n",
            "2019-10-29T19:54:24.501317: step 21, loss 0.736275, acc 0.453125\n",
            "2019-10-29T19:54:25.116411: step 22, loss 0.7465, acc 0.515625\n",
            "2019-10-29T19:54:25.750208: step 23, loss 0.746476, acc 0.4375\n",
            "2019-10-29T19:54:26.368841: step 24, loss 0.716491, acc 0.5\n",
            "2019-10-29T19:54:26.988887: step 25, loss 0.670153, acc 0.59375\n",
            "2019-10-29T19:54:27.604838: step 26, loss 0.708802, acc 0.515625\n",
            "2019-10-29T19:54:28.212332: step 27, loss 0.708308, acc 0.53125\n",
            "2019-10-29T19:54:28.827624: step 28, loss 0.662761, acc 0.625\n",
            "2019-10-29T19:54:29.435822: step 29, loss 0.744908, acc 0.484375\n",
            "2019-10-29T19:54:30.049839: step 30, loss 0.684098, acc 0.5625\n",
            "2019-10-29T19:54:30.668309: step 31, loss 0.758217, acc 0.46875\n",
            "2019-10-29T19:54:31.276921: step 32, loss 0.703987, acc 0.5625\n",
            "2019-10-29T19:54:31.888543: step 33, loss 0.746397, acc 0.421875\n",
            "2019-10-29T19:54:32.511077: step 34, loss 0.697669, acc 0.53125\n",
            "2019-10-29T19:54:33.119746: step 35, loss 0.699156, acc 0.484375\n",
            "2019-10-29T19:54:33.769157: step 36, loss 0.690201, acc 0.546875\n",
            "2019-10-29T19:54:34.384938: step 37, loss 0.684362, acc 0.515625\n",
            "2019-10-29T19:54:34.995930: step 38, loss 0.700439, acc 0.5\n",
            "2019-10-29T19:54:35.615171: step 39, loss 0.709816, acc 0.515625\n",
            "2019-10-29T19:54:36.232035: step 40, loss 0.69449, acc 0.515625\n",
            "2019-10-29T19:54:36.854138: step 41, loss 0.681959, acc 0.5625\n",
            "2019-10-29T19:54:37.468811: step 42, loss 0.702545, acc 0.515625\n",
            "2019-10-29T19:54:38.087535: step 43, loss 0.688507, acc 0.53125\n",
            "2019-10-29T19:54:38.696193: step 44, loss 0.679678, acc 0.578125\n",
            "2019-10-29T19:54:39.306404: step 45, loss 0.685712, acc 0.5625\n",
            "2019-10-29T19:54:39.925554: step 46, loss 0.688247, acc 0.484375\n",
            "2019-10-29T19:54:40.560555: step 47, loss 0.678925, acc 0.546875\n",
            "2019-10-29T19:54:41.179657: step 48, loss 0.731322, acc 0.484375\n",
            "2019-10-29T19:54:41.800568: step 49, loss 0.743564, acc 0.375\n",
            "2019-10-29T19:54:42.420788: step 50, loss 0.677466, acc 0.5\n",
            "2019-10-29T19:54:43.038942: step 51, loss 0.687153, acc 0.5\n",
            "2019-10-29T19:54:43.660660: step 52, loss 0.699242, acc 0.5\n",
            "2019-10-29T19:54:44.313640: step 53, loss 0.7004, acc 0.484375\n",
            "2019-10-29T19:54:44.925056: step 54, loss 0.685292, acc 0.5625\n",
            "2019-10-29T19:54:45.553094: step 55, loss 0.729732, acc 0.40625\n",
            "2019-10-29T19:54:46.171735: step 56, loss 0.689496, acc 0.578125\n",
            "2019-10-29T19:54:46.781723: step 57, loss 0.709525, acc 0.453125\n",
            "2019-10-29T19:54:47.395253: step 58, loss 0.694186, acc 0.46875\n",
            "2019-10-29T19:54:48.009975: step 59, loss 0.699289, acc 0.4375\n",
            "2019-10-29T19:54:48.617103: step 60, loss 0.73052, acc 0.40625\n",
            "2019-10-29T19:54:49.233328: step 61, loss 0.704679, acc 0.484375\n",
            "2019-10-29T19:54:49.850007: step 62, loss 0.69794, acc 0.53125\n",
            "2019-10-29T19:54:50.473127: step 63, loss 0.70878, acc 0.515625\n",
            "2019-10-29T19:54:51.087818: step 64, loss 0.703459, acc 0.5\n",
            "2019-10-29T19:54:51.698846: step 65, loss 0.704713, acc 0.515625\n",
            "2019-10-29T19:54:52.330322: step 66, loss 0.691529, acc 0.46875\n",
            "2019-10-29T19:54:52.946045: step 67, loss 0.690913, acc 0.515625\n",
            "2019-10-29T19:54:53.563108: step 68, loss 0.69403, acc 0.5\n",
            "2019-10-29T19:54:54.174884: step 69, loss 0.690683, acc 0.59375\n",
            "2019-10-29T19:54:54.812340: step 70, loss 0.696945, acc 0.53125\n",
            "2019-10-29T19:54:55.424941: step 71, loss 0.685721, acc 0.59375\n",
            "2019-10-29T19:54:56.039638: step 72, loss 0.683412, acc 0.625\n",
            "2019-10-29T19:54:56.646885: step 73, loss 0.699879, acc 0.546875\n",
            "2019-10-29T19:54:57.279283: step 74, loss 0.700448, acc 0.5\n",
            "2019-10-29T19:54:57.896064: step 75, loss 0.686963, acc 0.59375\n",
            "2019-10-29T19:54:58.551587: step 76, loss 0.70907, acc 0.5\n",
            "2019-10-29T19:54:59.173405: step 77, loss 0.716473, acc 0.4375\n",
            "2019-10-29T19:54:59.799848: step 78, loss 0.667435, acc 0.59375\n",
            "2019-10-29T19:55:00.418090: step 79, loss 0.700816, acc 0.515625\n",
            "2019-10-29T19:55:01.045674: step 80, loss 0.686544, acc 0.5\n",
            "2019-10-29T19:55:01.661021: step 81, loss 0.693211, acc 0.5\n",
            "2019-10-29T19:55:02.275109: step 82, loss 0.701806, acc 0.4375\n",
            "2019-10-29T19:55:02.887370: step 83, loss 0.693563, acc 0.546875\n",
            "2019-10-29T19:55:03.498912: step 84, loss 0.67913, acc 0.515625\n",
            "2019-10-29T19:55:04.110501: step 85, loss 0.69887, acc 0.5\n",
            "2019-10-29T19:55:04.727189: step 86, loss 0.67798, acc 0.609375\n",
            "2019-10-29T19:55:05.355686: step 87, loss 0.690441, acc 0.546875\n",
            "2019-10-29T19:55:05.970539: step 88, loss 0.698746, acc 0.484375\n",
            "2019-10-29T19:55:06.581299: step 89, loss 0.679718, acc 0.5625\n",
            "2019-10-29T19:55:07.191624: step 90, loss 0.711109, acc 0.46875\n",
            "2019-10-29T19:55:07.809737: step 91, loss 0.70489, acc 0.46875\n",
            "2019-10-29T19:55:08.434094: step 92, loss 0.691215, acc 0.546875\n",
            "2019-10-29T19:55:09.046886: step 93, loss 0.691916, acc 0.484375\n",
            "2019-10-29T19:55:09.662577: step 94, loss 0.69831, acc 0.59375\n",
            "2019-10-29T19:55:10.288821: step 95, loss 0.694324, acc 0.5625\n",
            "2019-10-29T19:55:10.928881: step 96, loss 0.701042, acc 0.5\n",
            "2019-10-29T19:55:11.545260: step 97, loss 0.697671, acc 0.40625\n",
            "2019-10-29T19:55:12.154320: step 98, loss 0.695113, acc 0.46875\n",
            "2019-10-29T19:55:12.771040: step 99, loss 0.684695, acc 0.515625\n",
            "2019-10-29T19:55:13.384393: step 100, loss 0.681374, acc 0.59375\n",
            "\n",
            "Evaluation:\n",
            "2019-10-29T19:55:13.842564: step 100, loss 0.69226, acc 0.534\n",
            "\n",
            "2019-10-29T19:55:14.460859: step 101, loss 0.68888, acc 0.46875\n",
            "2019-10-29T19:55:15.082436: step 102, loss 0.660895, acc 0.671875\n",
            "2019-10-29T19:55:15.701754: step 103, loss 0.708873, acc 0.484375\n",
            "2019-10-29T19:55:16.342449: step 104, loss 0.673064, acc 0.578125\n",
            "2019-10-29T19:55:16.964245: step 105, loss 0.702317, acc 0.53125\n",
            "2019-10-29T19:55:17.586786: step 106, loss 0.664068, acc 0.609375\n",
            "2019-10-29T19:55:18.199522: step 107, loss 0.72533, acc 0.5\n",
            "2019-10-29T19:55:18.817963: step 108, loss 0.681306, acc 0.578125\n",
            "2019-10-29T19:55:19.434019: step 109, loss 0.694571, acc 0.515625\n",
            "2019-10-29T19:55:20.041576: step 110, loss 0.685917, acc 0.515625\n",
            "2019-10-29T19:55:20.661062: step 111, loss 0.721866, acc 0.5\n",
            "2019-10-29T19:55:21.279173: step 112, loss 0.66522, acc 0.640625\n",
            "2019-10-29T19:55:21.901380: step 113, loss 0.701144, acc 0.453125\n",
            "2019-10-29T19:55:22.515957: step 114, loss 0.674517, acc 0.546875\n",
            "2019-10-29T19:55:23.129174: step 115, loss 0.704741, acc 0.4375\n",
            "2019-10-29T19:55:23.741056: step 116, loss 0.719472, acc 0.5\n",
            "2019-10-29T19:55:24.354215: step 117, loss 0.694973, acc 0.5625\n",
            "2019-10-29T19:55:24.967313: step 118, loss 0.718537, acc 0.46875\n",
            "2019-10-29T19:55:25.590720: step 119, loss 0.691152, acc 0.53125\n",
            "2019-10-29T19:55:26.220608: step 120, loss 0.727173, acc 0.375\n",
            "2019-10-29T19:55:26.855086: step 121, loss 0.688412, acc 0.53125\n",
            "2019-10-29T19:55:27.471830: step 122, loss 0.698822, acc 0.5\n",
            "2019-10-29T19:55:28.094646: step 123, loss 0.700425, acc 0.5\n",
            "2019-10-29T19:55:28.717546: step 124, loss 0.688598, acc 0.53125\n",
            "2019-10-29T19:55:29.338527: step 125, loss 0.709904, acc 0.46875\n",
            "2019-10-29T19:55:29.967561: step 126, loss 0.704878, acc 0.453125\n",
            "2019-10-29T19:55:30.584616: step 127, loss 0.723915, acc 0.40625\n",
            "2019-10-29T19:55:31.209738: step 128, loss 0.703052, acc 0.453125\n",
            "2019-10-29T19:55:31.838879: step 129, loss 0.690192, acc 0.578125\n",
            "2019-10-29T19:55:32.455758: step 130, loss 0.706082, acc 0.484375\n",
            "2019-10-29T19:55:33.065376: step 131, loss 0.695795, acc 0.5\n",
            "2019-10-29T19:55:33.686322: step 132, loss 0.699042, acc 0.5\n",
            "2019-10-29T19:55:34.314584: step 133, loss 0.704361, acc 0.484375\n",
            "2019-10-29T19:55:34.934082: step 134, loss 0.698657, acc 0.5\n",
            "2019-10-29T19:55:35.549870: step 135, loss 0.687291, acc 0.53125\n",
            "2019-10-29T19:55:36.181453: step 136, loss 0.69363, acc 0.5\n",
            "2019-10-29T19:55:36.817763: step 137, loss 0.681713, acc 0.578125\n",
            "2019-10-29T19:55:37.454165: step 138, loss 0.696796, acc 0.53125\n",
            "2019-10-29T19:55:38.067680: step 139, loss 0.715511, acc 0.390625\n",
            "2019-10-29T19:55:38.683411: step 140, loss 0.692639, acc 0.484375\n",
            "2019-10-29T19:55:39.271176: step 141, loss 0.688882, acc 0.5\n",
            "2019-10-29T19:55:39.897387: step 142, loss 0.683649, acc 0.578125\n",
            "2019-10-29T19:55:40.516673: step 143, loss 0.679487, acc 0.609375\n",
            "2019-10-29T19:55:41.130118: step 144, loss 0.699077, acc 0.453125\n",
            "2019-10-29T19:55:41.760160: step 145, loss 0.715976, acc 0.46875\n",
            "2019-10-29T19:55:42.394721: step 146, loss 0.687725, acc 0.5625\n",
            "2019-10-29T19:55:43.009114: step 147, loss 0.684014, acc 0.546875\n",
            "2019-10-29T19:55:43.626217: step 148, loss 0.669923, acc 0.703125\n",
            "2019-10-29T19:55:44.250012: step 149, loss 0.677034, acc 0.578125\n",
            "2019-10-29T19:55:44.873588: step 150, loss 0.683719, acc 0.578125\n",
            "2019-10-29T19:55:45.494597: step 151, loss 0.716186, acc 0.46875\n",
            "2019-10-29T19:55:46.108443: step 152, loss 0.70296, acc 0.4375\n",
            "2019-10-29T19:55:46.723098: step 153, loss 0.665037, acc 0.59375\n",
            "2019-10-29T19:55:47.343309: step 154, loss 0.694146, acc 0.5\n",
            "2019-10-29T19:55:47.974848: step 155, loss 0.658241, acc 0.578125\n",
            "2019-10-29T19:55:48.596278: step 156, loss 0.695048, acc 0.515625\n",
            "2019-10-29T19:55:49.213356: step 157, loss 0.710562, acc 0.53125\n",
            "2019-10-29T19:55:49.830400: step 158, loss 0.705517, acc 0.5\n",
            "2019-10-29T19:55:50.448178: step 159, loss 0.681374, acc 0.609375\n",
            "2019-10-29T19:55:51.069819: step 160, loss 0.665752, acc 0.671875\n",
            "2019-10-29T19:55:51.684970: step 161, loss 0.682051, acc 0.609375\n",
            "2019-10-29T19:55:52.305664: step 162, loss 0.678218, acc 0.5625\n",
            "2019-10-29T19:55:52.916195: step 163, loss 0.677415, acc 0.5\n",
            "2019-10-29T19:55:53.532634: step 164, loss 0.689205, acc 0.53125\n",
            "2019-10-29T19:55:54.143548: step 165, loss 0.680253, acc 0.546875\n",
            "2019-10-29T19:55:54.754355: step 166, loss 0.735781, acc 0.421875\n",
            "2019-10-29T19:55:55.377434: step 167, loss 0.706577, acc 0.453125\n",
            "2019-10-29T19:55:55.999851: step 168, loss 0.705256, acc 0.578125\n",
            "2019-10-29T19:55:56.624367: step 169, loss 0.70328, acc 0.484375\n",
            "2019-10-29T19:55:57.239786: step 170, loss 0.701679, acc 0.53125\n",
            "2019-10-29T19:55:57.862710: step 171, loss 0.67001, acc 0.5625\n",
            "2019-10-29T19:55:58.495160: step 172, loss 0.681464, acc 0.53125\n",
            "2019-10-29T19:55:59.143145: step 173, loss 0.706076, acc 0.5625\n",
            "2019-10-29T19:55:59.759789: step 174, loss 0.683345, acc 0.546875\n",
            "2019-10-29T19:56:00.373644: step 175, loss 0.728516, acc 0.4375\n",
            "2019-10-29T19:56:00.985102: step 176, loss 0.680321, acc 0.578125\n",
            "2019-10-29T19:56:01.606123: step 177, loss 0.719819, acc 0.421875\n",
            "2019-10-29T19:56:02.235890: step 178, loss 0.67027, acc 0.6875\n",
            "2019-10-29T19:56:02.847026: step 179, loss 0.693249, acc 0.53125\n",
            "2019-10-29T19:56:03.465706: step 180, loss 0.710712, acc 0.421875\n",
            "2019-10-29T19:56:04.081205: step 181, loss 0.714323, acc 0.453125\n",
            "2019-10-29T19:56:04.706864: step 182, loss 0.689663, acc 0.5\n",
            "2019-10-29T19:56:05.327559: step 183, loss 0.692554, acc 0.515625\n",
            "2019-10-29T19:56:05.940203: step 184, loss 0.681933, acc 0.609375\n",
            "2019-10-29T19:56:06.560762: step 185, loss 0.69855, acc 0.4375\n",
            "2019-10-29T19:56:07.174825: step 186, loss 0.716943, acc 0.453125\n",
            "2019-10-29T19:56:07.789850: step 187, loss 0.680151, acc 0.546875\n",
            "2019-10-29T19:56:08.405643: step 188, loss 0.686978, acc 0.53125\n",
            "2019-10-29T19:56:09.023639: step 189, loss 0.68807, acc 0.5625\n",
            "2019-10-29T19:56:09.661670: step 190, loss 0.682064, acc 0.53125\n",
            "2019-10-29T19:56:10.286961: step 191, loss 0.682508, acc 0.59375\n",
            "2019-10-29T19:56:10.916872: step 192, loss 0.715573, acc 0.546875\n",
            "2019-10-29T19:56:11.540998: step 193, loss 0.704624, acc 0.5\n",
            "2019-10-29T19:56:12.155598: step 194, loss 0.72565, acc 0.453125\n",
            "2019-10-29T19:56:12.789837: step 195, loss 0.693941, acc 0.5625\n",
            "2019-10-29T19:56:13.400112: step 196, loss 0.656924, acc 0.625\n",
            "2019-10-29T19:56:14.014750: step 197, loss 0.673041, acc 0.59375\n",
            "2019-10-29T19:56:14.635587: step 198, loss 0.692623, acc 0.5625\n",
            "2019-10-29T19:56:15.248947: step 199, loss 0.713183, acc 0.5\n",
            "2019-10-29T19:56:15.871520: step 200, loss 0.745505, acc 0.484375\n",
            "\n",
            "Evaluation:\n",
            "2019-10-29T19:56:16.256981: step 200, loss 0.731786, acc 0.482\n",
            "\n",
            "2019-10-29T19:56:16.877228: step 201, loss 0.745506, acc 0.453125\n",
            "2019-10-29T19:56:17.502689: step 202, loss 0.719608, acc 0.53125\n",
            "2019-10-29T19:56:18.115216: step 203, loss 0.704075, acc 0.484375\n",
            "2019-10-29T19:56:18.727917: step 204, loss 0.705512, acc 0.5625\n",
            "2019-10-29T19:56:19.348893: step 205, loss 0.637661, acc 0.6875\n",
            "2019-10-29T19:56:19.976554: step 206, loss 0.716974, acc 0.515625\n",
            "2019-10-29T19:56:20.600387: step 207, loss 0.73981, acc 0.46875\n",
            "2019-10-29T19:56:21.215717: step 208, loss 0.708776, acc 0.515625\n",
            "2019-10-29T19:56:21.834387: step 209, loss 0.702457, acc 0.5\n",
            "2019-10-29T19:56:22.465608: step 210, loss 0.732657, acc 0.5\n",
            "2019-10-29T19:56:23.082522: step 211, loss 0.663724, acc 0.59375\n",
            "2019-10-29T19:56:23.695802: step 212, loss 0.67761, acc 0.5\n",
            "2019-10-29T19:56:24.314039: step 213, loss 0.652582, acc 0.59375\n",
            "2019-10-29T19:56:24.936617: step 214, loss 0.720074, acc 0.5\n",
            "2019-10-29T19:56:25.556517: step 215, loss 0.714794, acc 0.40625\n",
            "2019-10-29T19:56:26.182441: step 216, loss 0.708238, acc 0.53125\n",
            "2019-10-29T19:56:26.802285: step 217, loss 0.683236, acc 0.484375\n",
            "2019-10-29T19:56:27.416282: step 218, loss 0.675535, acc 0.609375\n",
            "2019-10-29T19:56:28.039874: step 219, loss 0.687574, acc 0.53125\n",
            "2019-10-29T19:56:28.651408: step 220, loss 0.651948, acc 0.703125\n",
            "2019-10-29T19:56:29.266358: step 221, loss 0.720079, acc 0.484375\n",
            "2019-10-29T19:56:29.887068: step 222, loss 0.660809, acc 0.609375\n",
            "2019-10-29T19:56:30.507793: step 223, loss 0.641938, acc 0.640625\n",
            "2019-10-29T19:56:31.145335: step 224, loss 0.700325, acc 0.53125\n",
            "2019-10-29T19:56:31.766132: step 225, loss 0.722156, acc 0.453125\n",
            "2019-10-29T19:56:32.380894: step 226, loss 0.698638, acc 0.5625\n",
            "2019-10-29T19:56:32.999300: step 227, loss 0.665095, acc 0.59375\n",
            "2019-10-29T19:56:33.618414: step 228, loss 0.689756, acc 0.484375\n",
            "2019-10-29T19:56:34.243451: step 229, loss 0.688152, acc 0.578125\n",
            "2019-10-29T19:56:34.858066: step 230, loss 0.685368, acc 0.5625\n",
            "2019-10-29T19:56:35.484271: step 231, loss 0.715843, acc 0.453125\n",
            "2019-10-29T19:56:36.105786: step 232, loss 0.703522, acc 0.4375\n",
            "2019-10-29T19:56:36.729753: step 233, loss 0.67557, acc 0.640625\n",
            "2019-10-29T19:56:37.356675: step 234, loss 0.697787, acc 0.453125\n",
            "2019-10-29T19:56:37.982733: step 235, loss 0.689864, acc 0.53125\n",
            "2019-10-29T19:56:38.615317: step 236, loss 0.666153, acc 0.640625\n",
            "2019-10-29T19:56:39.240257: step 237, loss 0.713292, acc 0.546875\n",
            "2019-10-29T19:56:39.857073: step 238, loss 0.701215, acc 0.53125\n",
            "2019-10-29T19:56:40.481051: step 239, loss 0.642075, acc 0.734375\n",
            "2019-10-29T19:56:41.121755: step 240, loss 0.710769, acc 0.546875\n",
            "2019-10-29T19:56:41.768830: step 241, loss 0.709525, acc 0.53125\n",
            "2019-10-29T19:56:42.410717: step 242, loss 0.712609, acc 0.484375\n",
            "2019-10-29T19:56:43.050696: step 243, loss 0.691553, acc 0.5625\n",
            "2019-10-29T19:56:43.689993: step 244, loss 0.668116, acc 0.515625\n",
            "2019-10-29T19:56:44.312796: step 245, loss 0.706052, acc 0.46875\n",
            "2019-10-29T19:56:44.949366: step 246, loss 0.702381, acc 0.46875\n",
            "2019-10-29T19:56:45.581329: step 247, loss 0.723488, acc 0.4375\n",
            "2019-10-29T19:56:46.212652: step 248, loss 0.738663, acc 0.4375\n",
            "2019-10-29T19:56:46.831696: step 249, loss 0.739466, acc 0.421875\n",
            "2019-10-29T19:56:47.480939: step 250, loss 0.691897, acc 0.484375\n",
            "2019-10-29T19:56:48.111946: step 251, loss 0.67504, acc 0.640625\n",
            "2019-10-29T19:56:48.737345: step 252, loss 0.691814, acc 0.5625\n",
            "2019-10-29T19:56:49.368170: step 253, loss 0.687859, acc 0.5\n",
            "2019-10-29T19:56:49.998135: step 254, loss 0.69091, acc 0.5\n",
            "2019-10-29T19:56:50.623428: step 255, loss 0.69601, acc 0.578125\n",
            "2019-10-29T19:56:51.246165: step 256, loss 0.688977, acc 0.5\n",
            "2019-10-29T19:56:51.869738: step 257, loss 0.667335, acc 0.578125\n",
            "2019-10-29T19:56:52.517314: step 258, loss 0.678254, acc 0.53125\n",
            "2019-10-29T19:56:53.126219: step 259, loss 0.693588, acc 0.546875\n",
            "2019-10-29T19:56:53.754750: step 260, loss 0.719807, acc 0.4375\n",
            "2019-10-29T19:56:54.363983: step 261, loss 0.66183, acc 0.578125\n",
            "2019-10-29T19:56:54.982707: step 262, loss 0.689493, acc 0.53125\n",
            "2019-10-29T19:56:55.600034: step 263, loss 0.676829, acc 0.5625\n",
            "2019-10-29T19:56:56.216612: step 264, loss 0.676842, acc 0.5625\n",
            "2019-10-29T19:56:56.836856: step 265, loss 0.680206, acc 0.59375\n",
            "2019-10-29T19:56:57.460109: step 266, loss 0.671494, acc 0.5625\n",
            "2019-10-29T19:56:58.075323: step 267, loss 0.64472, acc 0.609375\n",
            "2019-10-29T19:56:58.689614: step 268, loss 0.739555, acc 0.453125\n",
            "2019-10-29T19:56:59.339922: step 269, loss 0.764176, acc 0.375\n",
            "2019-10-29T19:56:59.955756: step 270, loss 0.680311, acc 0.53125\n",
            "2019-10-29T19:57:00.575215: step 271, loss 0.650017, acc 0.625\n",
            "2019-10-29T19:57:01.195848: step 272, loss 0.68751, acc 0.5625\n",
            "2019-10-29T19:57:01.819711: step 273, loss 0.681179, acc 0.59375\n",
            "2019-10-29T19:57:02.437871: step 274, loss 0.673647, acc 0.59375\n",
            "2019-10-29T19:57:03.082696: step 275, loss 0.710361, acc 0.453125\n",
            "2019-10-29T19:57:03.703166: step 276, loss 0.652859, acc 0.65625\n",
            "2019-10-29T19:57:04.329611: step 277, loss 0.658779, acc 0.65625\n",
            "2019-10-29T19:57:04.950225: step 278, loss 0.72362, acc 0.46875\n",
            "2019-10-29T19:57:05.569265: step 279, loss 0.718981, acc 0.484375\n",
            "2019-10-29T19:57:06.190219: step 280, loss 0.660709, acc 0.609375\n",
            "2019-10-29T19:57:06.811340: step 281, loss 0.671318, acc 0.546875\n",
            "2019-10-29T19:57:07.400222: step 282, loss 0.685257, acc 0.5\n",
            "2019-10-29T19:57:08.023621: step 283, loss 0.665912, acc 0.546875\n",
            "2019-10-29T19:57:08.650416: step 284, loss 0.686849, acc 0.546875\n",
            "2019-10-29T19:57:09.265805: step 285, loss 0.682267, acc 0.546875\n",
            "2019-10-29T19:57:09.890727: step 286, loss 0.707197, acc 0.5\n",
            "2019-10-29T19:57:10.507417: step 287, loss 0.712535, acc 0.546875\n",
            "2019-10-29T19:57:11.124396: step 288, loss 0.751717, acc 0.421875\n",
            "2019-10-29T19:57:11.740354: step 289, loss 0.648829, acc 0.578125\n",
            "2019-10-29T19:57:12.355922: step 290, loss 0.660427, acc 0.578125\n",
            "2019-10-29T19:57:12.975765: step 291, loss 0.696302, acc 0.53125\n",
            "2019-10-29T19:57:13.603725: step 292, loss 0.716816, acc 0.515625\n",
            "2019-10-29T19:57:14.214565: step 293, loss 0.721404, acc 0.484375\n",
            "2019-10-29T19:57:14.839361: step 294, loss 0.636354, acc 0.65625\n",
            "2019-10-29T19:57:15.463860: step 295, loss 0.691817, acc 0.5\n",
            "2019-10-29T19:57:16.078742: step 296, loss 0.693922, acc 0.578125\n",
            "2019-10-29T19:57:16.696880: step 297, loss 0.652639, acc 0.609375\n",
            "2019-10-29T19:57:17.322000: step 298, loss 0.667948, acc 0.578125\n",
            "2019-10-29T19:57:17.939958: step 299, loss 0.690055, acc 0.46875\n",
            "2019-10-29T19:57:18.586130: step 300, loss 0.645979, acc 0.6875\n",
            "\n",
            "Evaluation:\n",
            "2019-10-29T19:57:18.981826: step 300, loss 0.696473, acc 0.526\n",
            "\n",
            "2019-10-29T19:57:19.612238: step 301, loss 0.695101, acc 0.546875\n",
            "2019-10-29T19:57:20.228384: step 302, loss 0.679264, acc 0.625\n",
            "2019-10-29T19:57:20.837978: step 303, loss 0.715419, acc 0.53125\n",
            "2019-10-29T19:57:21.461719: step 304, loss 0.657628, acc 0.546875\n",
            "2019-10-29T19:57:22.074230: step 305, loss 0.730866, acc 0.46875\n",
            "2019-10-29T19:57:22.702829: step 306, loss 0.667206, acc 0.53125\n",
            "2019-10-29T19:57:23.315940: step 307, loss 0.686342, acc 0.578125\n",
            "2019-10-29T19:57:23.948580: step 308, loss 0.658241, acc 0.53125\n",
            "2019-10-29T19:57:24.584751: step 309, loss 0.665209, acc 0.578125\n",
            "2019-10-29T19:57:25.203789: step 310, loss 0.693138, acc 0.515625\n",
            "2019-10-29T19:57:25.825505: step 311, loss 0.685593, acc 0.578125\n",
            "2019-10-29T19:57:26.449172: step 312, loss 0.621434, acc 0.671875\n",
            "2019-10-29T19:57:27.063719: step 313, loss 0.694887, acc 0.53125\n",
            "2019-10-29T19:57:27.682770: step 314, loss 0.634558, acc 0.625\n",
            "2019-10-29T19:57:28.302890: step 315, loss 0.658332, acc 0.625\n",
            "2019-10-29T19:57:28.921172: step 316, loss 0.68676, acc 0.578125\n",
            "2019-10-29T19:57:29.537283: step 317, loss 0.669244, acc 0.625\n",
            "2019-10-29T19:57:30.160156: step 318, loss 0.717373, acc 0.5\n",
            "2019-10-29T19:57:30.783922: step 319, loss 0.681476, acc 0.578125\n",
            "2019-10-29T19:57:31.402613: step 320, loss 0.700006, acc 0.46875\n",
            "2019-10-29T19:57:32.038450: step 321, loss 0.63842, acc 0.625\n",
            "2019-10-29T19:57:32.656166: step 322, loss 0.673344, acc 0.59375\n",
            "2019-10-29T19:57:33.270448: step 323, loss 0.670738, acc 0.578125\n",
            "2019-10-29T19:57:33.897834: step 324, loss 0.658658, acc 0.65625\n",
            "2019-10-29T19:57:34.534239: step 325, loss 0.679615, acc 0.625\n",
            "2019-10-29T19:57:35.151072: step 326, loss 0.691214, acc 0.5625\n",
            "2019-10-29T19:57:35.768126: step 327, loss 0.732702, acc 0.46875\n",
            "2019-10-29T19:57:36.382832: step 328, loss 0.677639, acc 0.59375\n",
            "2019-10-29T19:57:36.998112: step 329, loss 0.692936, acc 0.609375\n",
            "2019-10-29T19:57:37.620812: step 330, loss 0.641359, acc 0.625\n",
            "2019-10-29T19:57:38.247589: step 331, loss 0.678854, acc 0.5\n",
            "2019-10-29T19:57:38.864880: step 332, loss 0.68893, acc 0.59375\n",
            "2019-10-29T19:57:39.484844: step 333, loss 0.673308, acc 0.546875\n",
            "2019-10-29T19:57:40.097537: step 334, loss 0.681541, acc 0.5625\n",
            "2019-10-29T19:57:40.720949: step 335, loss 0.644782, acc 0.65625\n",
            "2019-10-29T19:57:41.341548: step 336, loss 0.744228, acc 0.421875\n",
            "2019-10-29T19:57:41.959598: step 337, loss 0.681498, acc 0.59375\n",
            "2019-10-29T19:57:42.574702: step 338, loss 0.693241, acc 0.53125\n",
            "2019-10-29T19:57:43.191237: step 339, loss 0.671806, acc 0.609375\n",
            "2019-10-29T19:57:43.854448: step 340, loss 0.681093, acc 0.578125\n",
            "2019-10-29T19:57:44.478314: step 341, loss 0.684836, acc 0.578125\n",
            "2019-10-29T19:57:45.127680: step 342, loss 0.714629, acc 0.5\n",
            "2019-10-29T19:57:45.747699: step 343, loss 0.644424, acc 0.671875\n",
            "2019-10-29T19:57:46.367206: step 344, loss 0.696638, acc 0.515625\n",
            "2019-10-29T19:57:46.981406: step 345, loss 0.651444, acc 0.609375\n",
            "2019-10-29T19:57:47.600697: step 346, loss 0.670949, acc 0.5625\n",
            "2019-10-29T19:57:48.218337: step 347, loss 0.697296, acc 0.546875\n",
            "2019-10-29T19:57:48.829719: step 348, loss 0.747249, acc 0.453125\n",
            "2019-10-29T19:57:49.449658: step 349, loss 0.685198, acc 0.5625\n",
            "2019-10-29T19:57:50.063645: step 350, loss 0.649511, acc 0.640625\n",
            "2019-10-29T19:57:50.684797: step 351, loss 0.71406, acc 0.46875\n",
            "2019-10-29T19:57:51.320342: step 352, loss 0.661575, acc 0.625\n",
            "2019-10-29T19:57:51.935578: step 353, loss 0.655943, acc 0.671875\n",
            "2019-10-29T19:57:52.553396: step 354, loss 0.706364, acc 0.578125\n",
            "2019-10-29T19:57:53.178694: step 355, loss 0.620944, acc 0.765625\n",
            "2019-10-29T19:57:53.797419: step 356, loss 0.662266, acc 0.609375\n",
            "2019-10-29T19:57:54.406775: step 357, loss 0.665499, acc 0.59375\n",
            "2019-10-29T19:57:55.029564: step 358, loss 0.693285, acc 0.546875\n",
            "2019-10-29T19:57:55.655126: step 359, loss 0.677841, acc 0.5\n",
            "2019-10-29T19:57:56.285845: step 360, loss 0.701134, acc 0.546875\n",
            "2019-10-29T19:57:56.910005: step 361, loss 0.695168, acc 0.5625\n",
            "2019-10-29T19:57:57.546530: step 362, loss 0.663342, acc 0.515625\n",
            "2019-10-29T19:57:58.169178: step 363, loss 0.672205, acc 0.5625\n",
            "2019-10-29T19:57:58.788174: step 364, loss 0.644119, acc 0.6875\n",
            "2019-10-29T19:57:59.421035: step 365, loss 0.687103, acc 0.53125\n",
            "2019-10-29T19:58:00.047109: step 366, loss 0.650703, acc 0.5625\n",
            "2019-10-29T19:58:00.660621: step 367, loss 0.716107, acc 0.46875\n",
            "2019-10-29T19:58:01.280065: step 368, loss 0.671058, acc 0.59375\n",
            "2019-10-29T19:58:01.907025: step 369, loss 0.684699, acc 0.5625\n",
            "2019-10-29T19:58:02.525755: step 370, loss 0.678733, acc 0.625\n",
            "2019-10-29T19:58:03.145278: step 371, loss 0.693655, acc 0.578125\n",
            "2019-10-29T19:58:03.762067: step 372, loss 0.655353, acc 0.625\n",
            "2019-10-29T19:58:04.386823: step 373, loss 0.666894, acc 0.609375\n",
            "2019-10-29T19:58:05.008146: step 374, loss 0.734952, acc 0.5\n",
            "2019-10-29T19:58:05.630951: step 375, loss 0.699318, acc 0.5625\n",
            "2019-10-29T19:58:06.280886: step 376, loss 0.700761, acc 0.5\n",
            "2019-10-29T19:58:06.904439: step 377, loss 0.66449, acc 0.578125\n",
            "2019-10-29T19:58:07.528653: step 378, loss 0.676393, acc 0.53125\n",
            "2019-10-29T19:58:08.141973: step 379, loss 0.671749, acc 0.609375\n",
            "2019-10-29T19:58:08.760691: step 380, loss 0.674531, acc 0.5625\n",
            "2019-10-29T19:58:09.400779: step 381, loss 0.65508, acc 0.640625\n",
            "2019-10-29T19:58:10.028085: step 382, loss 0.659045, acc 0.625\n",
            "2019-10-29T19:58:10.647836: step 383, loss 0.659333, acc 0.609375\n",
            "2019-10-29T19:58:11.268072: step 384, loss 0.665827, acc 0.578125\n",
            "2019-10-29T19:58:11.882942: step 385, loss 0.666006, acc 0.609375\n",
            "2019-10-29T19:58:12.497810: step 386, loss 0.681721, acc 0.5625\n",
            "2019-10-29T19:58:13.128715: step 387, loss 0.671502, acc 0.578125\n",
            "2019-10-29T19:58:13.746427: step 388, loss 0.675694, acc 0.53125\n",
            "2019-10-29T19:58:14.354994: step 389, loss 0.684967, acc 0.5625\n",
            "2019-10-29T19:58:14.990056: step 390, loss 0.651327, acc 0.609375\n",
            "2019-10-29T19:58:15.610106: step 391, loss 0.679499, acc 0.59375\n",
            "2019-10-29T19:58:16.228635: step 392, loss 0.664651, acc 0.640625\n",
            "2019-10-29T19:58:16.866269: step 393, loss 0.669732, acc 0.59375\n",
            "2019-10-29T19:58:17.492943: step 394, loss 0.597456, acc 0.71875\n",
            "2019-10-29T19:58:18.106549: step 395, loss 0.6501, acc 0.578125\n",
            "2019-10-29T19:58:18.728013: step 396, loss 0.709122, acc 0.546875\n",
            "2019-10-29T19:58:19.344112: step 397, loss 0.64559, acc 0.671875\n",
            "2019-10-29T19:58:19.967357: step 398, loss 0.638802, acc 0.609375\n",
            "2019-10-29T19:58:20.591877: step 399, loss 0.683439, acc 0.625\n",
            "2019-10-29T19:58:21.205729: step 400, loss 0.645338, acc 0.625\n",
            "\n",
            "Evaluation:\n",
            "2019-10-29T19:58:21.575971: step 400, loss 0.677032, acc 0.563\n",
            "\n",
            "2019-10-29T19:58:22.198376: step 401, loss 0.611572, acc 0.734375\n",
            "2019-10-29T19:58:22.817348: step 402, loss 0.68598, acc 0.671875\n",
            "2019-10-29T19:58:23.443240: step 403, loss 0.661812, acc 0.578125\n",
            "2019-10-29T19:58:24.077029: step 404, loss 0.664574, acc 0.5625\n",
            "2019-10-29T19:58:24.695393: step 405, loss 0.638548, acc 0.640625\n",
            "2019-10-29T19:58:25.319516: step 406, loss 0.592824, acc 0.75\n",
            "2019-10-29T19:58:25.937234: step 407, loss 0.598554, acc 0.59375\n",
            "2019-10-29T19:58:26.554184: step 408, loss 0.6564, acc 0.546875\n",
            "2019-10-29T19:58:27.172304: step 409, loss 0.60047, acc 0.640625\n",
            "2019-10-29T19:58:27.815873: step 410, loss 0.613567, acc 0.5625\n",
            "2019-10-29T19:58:28.430677: step 411, loss 0.648975, acc 0.640625\n",
            "2019-10-29T19:58:29.039733: step 412, loss 0.582933, acc 0.6875\n",
            "2019-10-29T19:58:29.657155: step 413, loss 0.604794, acc 0.671875\n",
            "2019-10-29T19:58:30.273986: step 414, loss 0.652299, acc 0.625\n",
            "2019-10-29T19:58:30.893744: step 415, loss 0.6907, acc 0.609375\n",
            "2019-10-29T19:58:31.532998: step 416, loss 0.640935, acc 0.59375\n",
            "2019-10-29T19:58:32.151810: step 417, loss 0.743003, acc 0.5\n",
            "2019-10-29T19:58:32.768810: step 418, loss 0.663151, acc 0.640625\n",
            "2019-10-29T19:58:33.385001: step 419, loss 0.728951, acc 0.53125\n",
            "2019-10-29T19:58:34.010973: step 420, loss 0.626256, acc 0.609375\n",
            "2019-10-29T19:58:34.639558: step 421, loss 0.650326, acc 0.65625\n",
            "2019-10-29T19:58:35.250676: step 422, loss 0.693404, acc 0.53125\n",
            "2019-10-29T19:58:35.846803: step 423, loss 0.781462, acc 0.55\n",
            "2019-10-29T19:58:36.474934: step 424, loss 0.669508, acc 0.59375\n",
            "2019-10-29T19:58:37.103797: step 425, loss 0.619701, acc 0.625\n",
            "2019-10-29T19:58:37.733597: step 426, loss 0.615181, acc 0.6875\n",
            "2019-10-29T19:58:38.377794: step 427, loss 0.688218, acc 0.5625\n",
            "2019-10-29T19:58:39.008206: step 428, loss 0.622592, acc 0.609375\n",
            "2019-10-29T19:58:39.626201: step 429, loss 0.633666, acc 0.671875\n",
            "2019-10-29T19:58:40.247198: step 430, loss 0.7037, acc 0.515625\n",
            "2019-10-29T19:58:40.881301: step 431, loss 0.65478, acc 0.65625\n",
            "2019-10-29T19:58:41.513070: step 432, loss 0.688601, acc 0.546875\n",
            "2019-10-29T19:58:42.138837: step 433, loss 0.603724, acc 0.703125\n",
            "2019-10-29T19:58:42.747392: step 434, loss 0.618175, acc 0.71875\n",
            "2019-10-29T19:58:43.364263: step 435, loss 0.617371, acc 0.703125\n",
            "2019-10-29T19:58:43.993073: step 436, loss 0.623423, acc 0.6875\n",
            "2019-10-29T19:58:44.623197: step 437, loss 0.617227, acc 0.703125\n",
            "2019-10-29T19:58:45.252999: step 438, loss 0.633607, acc 0.65625\n",
            "2019-10-29T19:58:45.872144: step 439, loss 0.690374, acc 0.546875\n",
            "2019-10-29T19:58:46.493931: step 440, loss 0.624108, acc 0.65625\n",
            "2019-10-29T19:58:47.111300: step 441, loss 0.623886, acc 0.671875\n",
            "2019-10-29T19:58:47.734993: step 442, loss 0.629974, acc 0.671875\n",
            "2019-10-29T19:58:48.355264: step 443, loss 0.678057, acc 0.59375\n",
            "2019-10-29T19:58:48.988564: step 444, loss 0.608438, acc 0.625\n",
            "2019-10-29T19:58:49.613818: step 445, loss 0.587278, acc 0.703125\n",
            "2019-10-29T19:58:50.235360: step 446, loss 0.562768, acc 0.703125\n",
            "2019-10-29T19:58:50.856405: step 447, loss 0.56298, acc 0.75\n",
            "2019-10-29T19:58:51.488158: step 448, loss 0.593283, acc 0.6875\n",
            "2019-10-29T19:58:52.105972: step 449, loss 0.657881, acc 0.640625\n",
            "2019-10-29T19:58:52.736224: step 450, loss 0.627122, acc 0.625\n",
            "2019-10-29T19:58:53.358770: step 451, loss 0.671142, acc 0.609375\n",
            "2019-10-29T19:58:53.977873: step 452, loss 0.578171, acc 0.71875\n",
            "2019-10-29T19:58:54.602484: step 453, loss 0.583006, acc 0.703125\n",
            "2019-10-29T19:58:55.224034: step 454, loss 0.58671, acc 0.71875\n",
            "2019-10-29T19:58:55.843917: step 455, loss 0.598266, acc 0.625\n",
            "2019-10-29T19:58:56.472720: step 456, loss 0.633639, acc 0.609375\n",
            "2019-10-29T19:58:57.087555: step 457, loss 0.634859, acc 0.609375\n",
            "2019-10-29T19:58:57.709676: step 458, loss 0.628857, acc 0.640625\n",
            "2019-10-29T19:58:58.328712: step 459, loss 0.629763, acc 0.640625\n",
            "2019-10-29T19:58:58.942221: step 460, loss 0.683896, acc 0.65625\n",
            "2019-10-29T19:58:59.585022: step 461, loss 0.612908, acc 0.6875\n",
            "2019-10-29T19:59:00.202661: step 462, loss 0.674396, acc 0.59375\n",
            "2019-10-29T19:59:00.820538: step 463, loss 0.579787, acc 0.671875\n",
            "2019-10-29T19:59:01.435370: step 464, loss 0.664674, acc 0.59375\n",
            "2019-10-29T19:59:02.051343: step 465, loss 0.575816, acc 0.71875\n",
            "2019-10-29T19:59:02.679997: step 466, loss 0.554366, acc 0.71875\n",
            "2019-10-29T19:59:03.308771: step 467, loss 0.66838, acc 0.625\n",
            "2019-10-29T19:59:03.931863: step 468, loss 0.55316, acc 0.6875\n",
            "2019-10-29T19:59:04.545165: step 469, loss 0.596978, acc 0.65625\n",
            "2019-10-29T19:59:05.164754: step 470, loss 0.561471, acc 0.71875\n",
            "2019-10-29T19:59:05.789516: step 471, loss 0.635259, acc 0.640625\n",
            "2019-10-29T19:59:06.405754: step 472, loss 0.538309, acc 0.8125\n",
            "2019-10-29T19:59:07.026143: step 473, loss 0.582555, acc 0.703125\n",
            "2019-10-29T19:59:07.646413: step 474, loss 0.6864, acc 0.59375\n",
            "2019-10-29T19:59:08.268754: step 475, loss 0.668482, acc 0.625\n",
            "2019-10-29T19:59:08.880312: step 476, loss 0.643922, acc 0.609375\n",
            "2019-10-29T19:59:09.503440: step 477, loss 0.595515, acc 0.75\n",
            "2019-10-29T19:59:10.129845: step 478, loss 0.614617, acc 0.71875\n",
            "2019-10-29T19:59:10.753585: step 479, loss 0.634795, acc 0.609375\n",
            "2019-10-29T19:59:11.360923: step 480, loss 0.624707, acc 0.703125\n",
            "2019-10-29T19:59:11.978601: step 481, loss 0.621483, acc 0.609375\n",
            "2019-10-29T19:59:12.590862: step 482, loss 0.529289, acc 0.734375\n",
            "2019-10-29T19:59:13.210723: step 483, loss 0.645859, acc 0.59375\n",
            "2019-10-29T19:59:13.838914: step 484, loss 0.596387, acc 0.71875\n",
            "2019-10-29T19:59:14.467001: step 485, loss 0.677818, acc 0.640625\n",
            "2019-10-29T19:59:15.095684: step 486, loss 0.566933, acc 0.71875\n",
            "2019-10-29T19:59:15.725126: step 487, loss 0.587484, acc 0.703125\n",
            "2019-10-29T19:59:16.348814: step 488, loss 0.524742, acc 0.734375\n",
            "2019-10-29T19:59:16.968277: step 489, loss 0.56483, acc 0.71875\n",
            "2019-10-29T19:59:17.583776: step 490, loss 0.582052, acc 0.6875\n",
            "2019-10-29T19:59:18.198659: step 491, loss 0.568613, acc 0.671875\n",
            "2019-10-29T19:59:18.816234: step 492, loss 0.695298, acc 0.578125\n",
            "2019-10-29T19:59:19.433818: step 493, loss 0.75934, acc 0.609375\n",
            "2019-10-29T19:59:20.048594: step 494, loss 0.648062, acc 0.609375\n",
            "2019-10-29T19:59:20.684650: step 495, loss 0.66005, acc 0.609375\n",
            "2019-10-29T19:59:21.298985: step 496, loss 0.510781, acc 0.78125\n",
            "2019-10-29T19:59:21.923344: step 497, loss 0.533839, acc 0.8125\n",
            "2019-10-29T19:59:22.549044: step 498, loss 0.581248, acc 0.734375\n",
            "2019-10-29T19:59:23.166426: step 499, loss 0.611706, acc 0.65625\n",
            "2019-10-29T19:59:23.800586: step 500, loss 0.609176, acc 0.640625\n",
            "\n",
            "Evaluation:\n",
            "2019-10-29T19:59:24.169307: step 500, loss 0.651168, acc 0.601\n",
            "\n",
            "2019-10-29T19:59:24.796401: step 501, loss 0.705827, acc 0.53125\n",
            "2019-10-29T19:59:25.418823: step 502, loss 0.608052, acc 0.671875\n",
            "2019-10-29T19:59:26.034373: step 503, loss 0.55993, acc 0.828125\n",
            "2019-10-29T19:59:26.646715: step 504, loss 0.60917, acc 0.6875\n",
            "2019-10-29T19:59:27.273910: step 505, loss 0.603768, acc 0.671875\n",
            "2019-10-29T19:59:27.891812: step 506, loss 0.621761, acc 0.640625\n",
            "2019-10-29T19:59:28.510536: step 507, loss 0.649025, acc 0.640625\n",
            "2019-10-29T19:59:29.130199: step 508, loss 0.583489, acc 0.703125\n",
            "2019-10-29T19:59:29.744800: step 509, loss 0.589353, acc 0.71875\n",
            "2019-10-29T19:59:30.371529: step 510, loss 0.614354, acc 0.640625\n",
            "2019-10-29T19:59:31.007842: step 511, loss 0.639382, acc 0.65625\n",
            "2019-10-29T19:59:31.624152: step 512, loss 0.653334, acc 0.640625\n",
            "2019-10-29T19:59:32.241250: step 513, loss 0.572715, acc 0.734375\n",
            "2019-10-29T19:59:32.850594: step 514, loss 0.606221, acc 0.609375\n",
            "2019-10-29T19:59:33.471123: step 515, loss 0.592216, acc 0.71875\n",
            "2019-10-29T19:59:34.084427: step 516, loss 0.636534, acc 0.640625\n",
            "2019-10-29T19:59:34.709185: step 517, loss 0.553389, acc 0.796875\n",
            "2019-10-29T19:59:35.328006: step 518, loss 0.545793, acc 0.65625\n",
            "2019-10-29T19:59:35.939982: step 519, loss 0.695795, acc 0.65625\n",
            "2019-10-29T19:59:36.562398: step 520, loss 0.630486, acc 0.65625\n",
            "2019-10-29T19:59:37.188190: step 521, loss 0.546295, acc 0.75\n",
            "2019-10-29T19:59:37.799021: step 522, loss 0.616312, acc 0.71875\n",
            "2019-10-29T19:59:38.415102: step 523, loss 0.525494, acc 0.75\n",
            "2019-10-29T19:59:39.034891: step 524, loss 0.555823, acc 0.65625\n",
            "2019-10-29T19:59:39.662322: step 525, loss 0.642891, acc 0.609375\n",
            "2019-10-29T19:59:40.286751: step 526, loss 0.594413, acc 0.65625\n",
            "2019-10-29T19:59:40.898274: step 527, loss 0.549763, acc 0.75\n",
            "2019-10-29T19:59:41.524942: step 528, loss 0.633606, acc 0.609375\n",
            "2019-10-29T19:59:42.149194: step 529, loss 0.576324, acc 0.65625\n",
            "2019-10-29T19:59:42.775376: step 530, loss 0.712069, acc 0.53125\n",
            "2019-10-29T19:59:43.391405: step 531, loss 0.6544, acc 0.59375\n",
            "2019-10-29T19:59:44.016815: step 532, loss 0.596294, acc 0.640625\n",
            "2019-10-29T19:59:44.637250: step 533, loss 0.582235, acc 0.71875\n",
            "2019-10-29T19:59:45.263377: step 534, loss 0.688527, acc 0.578125\n",
            "2019-10-29T19:59:45.894390: step 535, loss 0.542075, acc 0.765625\n",
            "2019-10-29T19:59:46.525674: step 536, loss 0.609708, acc 0.625\n",
            "2019-10-29T19:59:47.146408: step 537, loss 0.630525, acc 0.609375\n",
            "2019-10-29T19:59:47.764196: step 538, loss 0.579107, acc 0.734375\n",
            "2019-10-29T19:59:48.381072: step 539, loss 0.615762, acc 0.734375\n",
            "2019-10-29T19:59:48.997388: step 540, loss 0.63337, acc 0.59375\n",
            "2019-10-29T19:59:49.614661: step 541, loss 0.642869, acc 0.625\n",
            "2019-10-29T19:59:50.235892: step 542, loss 0.584419, acc 0.703125\n",
            "2019-10-29T19:59:50.871134: step 543, loss 0.674506, acc 0.5625\n",
            "2019-10-29T19:59:51.489930: step 544, loss 0.646695, acc 0.625\n",
            "2019-10-29T19:59:52.113555: step 545, loss 0.623663, acc 0.609375\n",
            "2019-10-29T19:59:52.746274: step 546, loss 0.626367, acc 0.625\n",
            "2019-10-29T19:59:53.365919: step 547, loss 0.524046, acc 0.71875\n",
            "2019-10-29T19:59:53.990165: step 548, loss 0.543384, acc 0.765625\n",
            "2019-10-29T19:59:54.617680: step 549, loss 0.629952, acc 0.671875\n",
            "2019-10-29T19:59:55.242317: step 550, loss 0.588088, acc 0.625\n",
            "2019-10-29T19:59:55.864677: step 551, loss 0.632721, acc 0.65625\n",
            "2019-10-29T19:59:56.487241: step 552, loss 0.602112, acc 0.65625\n",
            "2019-10-29T19:59:57.105671: step 553, loss 0.597393, acc 0.625\n",
            "2019-10-29T19:59:57.732246: step 554, loss 0.516247, acc 0.734375\n",
            "2019-10-29T19:59:58.344968: step 555, loss 0.583437, acc 0.703125\n",
            "2019-10-29T19:59:58.965238: step 556, loss 0.656676, acc 0.625\n",
            "2019-10-29T19:59:59.584877: step 557, loss 0.592332, acc 0.671875\n",
            "2019-10-29T20:00:00.202072: step 558, loss 0.496944, acc 0.75\n",
            "2019-10-29T20:00:00.824630: step 559, loss 0.651465, acc 0.625\n",
            "2019-10-29T20:00:01.450996: step 560, loss 0.591893, acc 0.71875\n",
            "2019-10-29T20:00:02.079011: step 561, loss 0.547057, acc 0.71875\n",
            "2019-10-29T20:00:02.699585: step 562, loss 0.628114, acc 0.625\n",
            "2019-10-29T20:00:03.334824: step 563, loss 0.592218, acc 0.640625\n",
            "2019-10-29T20:00:03.939095: step 564, loss 0.673128, acc 0.625\n",
            "2019-10-29T20:00:04.567298: step 565, loss 0.623269, acc 0.640625\n",
            "2019-10-29T20:00:05.195098: step 566, loss 0.561579, acc 0.71875\n",
            "2019-10-29T20:00:05.821312: step 567, loss 0.559412, acc 0.703125\n",
            "2019-10-29T20:00:06.440427: step 568, loss 0.546322, acc 0.734375\n",
            "2019-10-29T20:00:07.059129: step 569, loss 0.497398, acc 0.765625\n",
            "2019-10-29T20:00:07.728436: step 570, loss 0.575177, acc 0.703125\n",
            "2019-10-29T20:00:08.362240: step 571, loss 0.502307, acc 0.75\n",
            "2019-10-29T20:00:08.981801: step 572, loss 0.524658, acc 0.765625\n",
            "2019-10-29T20:00:09.599668: step 573, loss 0.544352, acc 0.71875\n",
            "2019-10-29T20:00:10.223363: step 574, loss 0.545587, acc 0.78125\n",
            "2019-10-29T20:00:10.841661: step 575, loss 0.440785, acc 0.78125\n",
            "2019-10-29T20:00:11.463336: step 576, loss 0.504866, acc 0.6875\n",
            "2019-10-29T20:00:12.080577: step 577, loss 0.513812, acc 0.78125\n",
            "2019-10-29T20:00:12.699596: step 578, loss 0.437595, acc 0.78125\n",
            "2019-10-29T20:00:13.329655: step 579, loss 0.541061, acc 0.734375\n",
            "2019-10-29T20:00:13.953679: step 580, loss 0.531479, acc 0.6875\n",
            "2019-10-29T20:00:14.574617: step 581, loss 0.496302, acc 0.703125\n",
            "2019-10-29T20:00:15.192044: step 582, loss 0.497172, acc 0.75\n",
            "2019-10-29T20:00:15.817185: step 583, loss 0.516512, acc 0.75\n",
            "2019-10-29T20:00:16.447895: step 584, loss 0.498122, acc 0.78125\n",
            "2019-10-29T20:00:17.064287: step 585, loss 0.579942, acc 0.6875\n",
            "2019-10-29T20:00:17.693848: step 586, loss 0.551001, acc 0.734375\n",
            "2019-10-29T20:00:18.312931: step 587, loss 0.516085, acc 0.765625\n",
            "2019-10-29T20:00:18.928624: step 588, loss 0.540485, acc 0.734375\n",
            "2019-10-29T20:00:19.544078: step 589, loss 0.445298, acc 0.796875\n",
            "2019-10-29T20:00:20.172342: step 590, loss 0.543519, acc 0.78125\n",
            "2019-10-29T20:00:20.793077: step 591, loss 0.50866, acc 0.71875\n",
            "2019-10-29T20:00:21.416800: step 592, loss 0.527934, acc 0.71875\n",
            "2019-10-29T20:00:22.039227: step 593, loss 0.559498, acc 0.734375\n",
            "2019-10-29T20:00:22.655993: step 594, loss 0.475125, acc 0.8125\n",
            "2019-10-29T20:00:23.282796: step 595, loss 0.555251, acc 0.75\n",
            "2019-10-29T20:00:23.905593: step 596, loss 0.459982, acc 0.8125\n",
            "2019-10-29T20:00:24.521633: step 597, loss 0.588092, acc 0.65625\n",
            "2019-10-29T20:00:25.148024: step 598, loss 0.606793, acc 0.71875\n",
            "2019-10-29T20:00:25.766180: step 599, loss 0.452866, acc 0.796875\n",
            "2019-10-29T20:00:26.388936: step 600, loss 0.5613, acc 0.6875\n",
            "\n",
            "Evaluation:\n",
            "2019-10-29T20:00:26.762999: step 600, loss 0.66977, acc 0.64\n",
            "\n",
            "2019-10-29T20:00:27.385797: step 601, loss 0.639065, acc 0.640625\n",
            "2019-10-29T20:00:28.002663: step 602, loss 0.546292, acc 0.703125\n",
            "2019-10-29T20:00:28.615783: step 603, loss 0.574633, acc 0.75\n",
            "2019-10-29T20:00:29.235004: step 604, loss 0.600134, acc 0.671875\n",
            "2019-10-29T20:00:29.853334: step 605, loss 0.54489, acc 0.6875\n",
            "2019-10-29T20:00:30.473841: step 606, loss 0.497981, acc 0.75\n",
            "2019-10-29T20:00:31.111416: step 607, loss 0.552078, acc 0.703125\n",
            "2019-10-29T20:00:31.722331: step 608, loss 0.496359, acc 0.75\n",
            "2019-10-29T20:00:32.344240: step 609, loss 0.525577, acc 0.8125\n",
            "2019-10-29T20:00:32.966318: step 610, loss 0.566459, acc 0.6875\n",
            "2019-10-29T20:00:33.586697: step 611, loss 0.485403, acc 0.78125\n",
            "2019-10-29T20:00:34.219619: step 612, loss 0.460459, acc 0.765625\n",
            "2019-10-29T20:00:34.851226: step 613, loss 0.588315, acc 0.734375\n",
            "2019-10-29T20:00:35.469853: step 614, loss 0.544766, acc 0.71875\n",
            "2019-10-29T20:00:36.091243: step 615, loss 0.455391, acc 0.78125\n",
            "2019-10-29T20:00:36.705537: step 616, loss 0.471373, acc 0.75\n",
            "2019-10-29T20:00:37.329649: step 617, loss 0.606768, acc 0.671875\n",
            "2019-10-29T20:00:37.946430: step 618, loss 0.382453, acc 0.859375\n",
            "2019-10-29T20:00:38.569311: step 619, loss 0.564201, acc 0.703125\n",
            "2019-10-29T20:00:39.186790: step 620, loss 0.517869, acc 0.765625\n",
            "2019-10-29T20:00:39.806324: step 621, loss 0.587068, acc 0.71875\n",
            "2019-10-29T20:00:40.427059: step 622, loss 0.531691, acc 0.78125\n",
            "2019-10-29T20:00:41.049368: step 623, loss 0.480913, acc 0.734375\n",
            "2019-10-29T20:00:41.731391: step 624, loss 0.47454, acc 0.796875\n",
            "2019-10-29T20:00:42.360020: step 625, loss 0.499528, acc 0.796875\n",
            "2019-10-29T20:00:42.975226: step 626, loss 0.5345, acc 0.734375\n",
            "2019-10-29T20:00:43.590326: step 627, loss 0.444894, acc 0.765625\n",
            "2019-10-29T20:00:44.206780: step 628, loss 0.505447, acc 0.78125\n",
            "2019-10-29T20:00:44.832839: step 629, loss 0.475686, acc 0.78125\n",
            "2019-10-29T20:00:45.449723: step 630, loss 0.599361, acc 0.671875\n",
            "2019-10-29T20:00:46.083338: step 631, loss 0.434862, acc 0.765625\n",
            "2019-10-29T20:00:46.707456: step 632, loss 0.442458, acc 0.78125\n",
            "2019-10-29T20:00:47.319104: step 633, loss 0.497499, acc 0.78125\n",
            "2019-10-29T20:00:47.950946: step 634, loss 0.670443, acc 0.640625\n",
            "2019-10-29T20:00:48.577936: step 635, loss 0.541536, acc 0.71875\n",
            "2019-10-29T20:00:49.198763: step 636, loss 0.588446, acc 0.703125\n",
            "2019-10-29T20:00:49.816849: step 637, loss 0.39769, acc 0.84375\n",
            "2019-10-29T20:00:50.444574: step 638, loss 0.379278, acc 0.84375\n",
            "2019-10-29T20:00:51.078656: step 639, loss 0.483069, acc 0.75\n",
            "2019-10-29T20:00:51.706880: step 640, loss 0.481971, acc 0.859375\n",
            "2019-10-29T20:00:52.331330: step 641, loss 0.408055, acc 0.84375\n",
            "2019-10-29T20:00:52.955908: step 642, loss 0.539095, acc 0.75\n",
            "2019-10-29T20:00:53.570441: step 643, loss 0.423299, acc 0.796875\n",
            "2019-10-29T20:00:54.183530: step 644, loss 0.472033, acc 0.71875\n",
            "2019-10-29T20:00:54.794644: step 645, loss 0.489339, acc 0.734375\n",
            "2019-10-29T20:00:55.405060: step 646, loss 0.712466, acc 0.59375\n",
            "2019-10-29T20:00:56.020714: step 647, loss 0.413545, acc 0.8125\n",
            "2019-10-29T20:00:56.642148: step 648, loss 0.612197, acc 0.6875\n",
            "2019-10-29T20:00:57.251081: step 649, loss 0.512025, acc 0.703125\n",
            "2019-10-29T20:00:57.867754: step 650, loss 0.4253, acc 0.8125\n",
            "2019-10-29T20:00:58.493860: step 651, loss 0.537547, acc 0.71875\n",
            "2019-10-29T20:00:59.110863: step 652, loss 0.433295, acc 0.78125\n",
            "2019-10-29T20:00:59.727761: step 653, loss 0.515096, acc 0.703125\n",
            "2019-10-29T20:01:00.368100: step 654, loss 0.522825, acc 0.71875\n",
            "2019-10-29T20:01:00.978785: step 655, loss 0.425316, acc 0.8125\n",
            "2019-10-29T20:01:01.598748: step 656, loss 0.459429, acc 0.765625\n",
            "2019-10-29T20:01:02.213248: step 657, loss 0.466285, acc 0.765625\n",
            "2019-10-29T20:01:02.831712: step 658, loss 0.427699, acc 0.828125\n",
            "2019-10-29T20:01:03.461637: step 659, loss 0.492163, acc 0.75\n",
            "2019-10-29T20:01:04.075755: step 660, loss 0.441052, acc 0.828125\n",
            "2019-10-29T20:01:04.695703: step 661, loss 0.564742, acc 0.6875\n",
            "2019-10-29T20:01:05.320268: step 662, loss 0.658194, acc 0.609375\n",
            "2019-10-29T20:01:05.956720: step 663, loss 0.591335, acc 0.671875\n",
            "2019-10-29T20:01:06.570582: step 664, loss 0.518717, acc 0.75\n",
            "2019-10-29T20:01:07.179021: step 665, loss 0.483534, acc 0.75\n",
            "2019-10-29T20:01:07.790920: step 666, loss 0.445036, acc 0.78125\n",
            "2019-10-29T20:01:08.414862: step 667, loss 0.501918, acc 0.78125\n",
            "2019-10-29T20:01:09.035567: step 668, loss 0.576427, acc 0.71875\n",
            "2019-10-29T20:01:09.661751: step 669, loss 0.409598, acc 0.859375\n",
            "2019-10-29T20:01:10.288492: step 670, loss 0.426848, acc 0.875\n",
            "2019-10-29T20:01:10.913779: step 671, loss 0.481581, acc 0.78125\n",
            "2019-10-29T20:01:11.534552: step 672, loss 0.565544, acc 0.71875\n",
            "2019-10-29T20:01:12.153422: step 673, loss 0.528824, acc 0.71875\n",
            "2019-10-29T20:01:12.762459: step 674, loss 0.521224, acc 0.75\n",
            "2019-10-29T20:01:13.385740: step 675, loss 0.543147, acc 0.765625\n",
            "2019-10-29T20:01:14.004515: step 676, loss 0.597642, acc 0.671875\n",
            "2019-10-29T20:01:14.617599: step 677, loss 0.529396, acc 0.75\n",
            "2019-10-29T20:01:15.232159: step 678, loss 0.602959, acc 0.6875\n",
            "2019-10-29T20:01:15.864780: step 679, loss 0.571905, acc 0.71875\n",
            "2019-10-29T20:01:16.487729: step 680, loss 0.382318, acc 0.875\n",
            "2019-10-29T20:01:17.106060: step 681, loss 0.486434, acc 0.765625\n",
            "2019-10-29T20:01:17.721300: step 682, loss 0.539396, acc 0.734375\n",
            "2019-10-29T20:01:18.346623: step 683, loss 0.583563, acc 0.671875\n",
            "2019-10-29T20:01:18.962038: step 684, loss 0.453274, acc 0.796875\n",
            "2019-10-29T20:01:19.614723: step 685, loss 0.394755, acc 0.84375\n",
            "2019-10-29T20:01:20.242080: step 686, loss 0.517723, acc 0.78125\n",
            "2019-10-29T20:01:20.860016: step 687, loss 0.586333, acc 0.734375\n",
            "2019-10-29T20:01:21.478390: step 688, loss 0.483683, acc 0.8125\n",
            "2019-10-29T20:01:22.088598: step 689, loss 0.473017, acc 0.78125\n",
            "2019-10-29T20:01:22.708089: step 690, loss 0.309443, acc 0.90625\n",
            "2019-10-29T20:01:23.338163: step 691, loss 0.92537, acc 0.625\n",
            "2019-10-29T20:01:23.954139: step 692, loss 0.570387, acc 0.71875\n",
            "2019-10-29T20:01:24.563191: step 693, loss 0.522382, acc 0.71875\n",
            "2019-10-29T20:01:25.179163: step 694, loss 0.461946, acc 0.796875\n",
            "2019-10-29T20:01:25.799103: step 695, loss 0.507203, acc 0.734375\n",
            "2019-10-29T20:01:26.403581: step 696, loss 0.586527, acc 0.640625\n",
            "2019-10-29T20:01:27.014657: step 697, loss 0.589792, acc 0.671875\n",
            "2019-10-29T20:01:27.629650: step 698, loss 0.456494, acc 0.859375\n",
            "2019-10-29T20:01:28.265234: step 699, loss 0.562225, acc 0.734375\n",
            "2019-10-29T20:01:28.891171: step 700, loss 0.54536, acc 0.75\n",
            "\n",
            "Evaluation:\n",
            "2019-10-29T20:01:29.258141: step 700, loss 0.616485, acc 0.658\n",
            "\n",
            "2019-10-29T20:01:29.880617: step 701, loss 0.515522, acc 0.8125\n",
            "2019-10-29T20:01:30.501392: step 702, loss 0.609328, acc 0.640625\n",
            "2019-10-29T20:01:31.117842: step 703, loss 0.568568, acc 0.65625\n",
            "2019-10-29T20:01:31.734144: step 704, loss 0.430008, acc 0.859375\n",
            "2019-10-29T20:01:32.324849: step 705, loss 0.454873, acc 0.85\n",
            "2019-10-29T20:01:32.947341: step 706, loss 0.523399, acc 0.796875\n",
            "2019-10-29T20:01:33.569589: step 707, loss 0.5069, acc 0.796875\n",
            "2019-10-29T20:01:34.199232: step 708, loss 0.467812, acc 0.8125\n",
            "2019-10-29T20:01:34.817168: step 709, loss 0.473979, acc 0.796875\n",
            "2019-10-29T20:01:35.433976: step 710, loss 0.466319, acc 0.78125\n",
            "2019-10-29T20:01:36.053276: step 711, loss 0.423669, acc 0.84375\n",
            "2019-10-29T20:01:36.676379: step 712, loss 0.336007, acc 0.890625\n",
            "2019-10-29T20:01:37.306148: step 713, loss 0.522853, acc 0.828125\n",
            "2019-10-29T20:01:37.921765: step 714, loss 0.495495, acc 0.78125\n",
            "2019-10-29T20:01:38.564026: step 715, loss 0.43424, acc 0.828125\n",
            "2019-10-29T20:01:39.194286: step 716, loss 0.403793, acc 0.828125\n",
            "2019-10-29T20:01:39.805661: step 717, loss 0.397996, acc 0.8125\n",
            "2019-10-29T20:01:40.424096: step 718, loss 0.562234, acc 0.75\n",
            "2019-10-29T20:01:41.049691: step 719, loss 0.433113, acc 0.8125\n",
            "2019-10-29T20:01:41.663281: step 720, loss 0.548423, acc 0.75\n",
            "2019-10-29T20:01:42.297797: step 721, loss 0.396687, acc 0.828125\n",
            "2019-10-29T20:01:42.918870: step 722, loss 0.436582, acc 0.78125\n",
            "2019-10-29T20:01:43.537876: step 723, loss 0.464249, acc 0.78125\n",
            "2019-10-29T20:01:44.157625: step 724, loss 0.503063, acc 0.734375\n",
            "2019-10-29T20:01:44.792596: step 725, loss 0.400738, acc 0.796875\n",
            "2019-10-29T20:01:45.418765: step 726, loss 0.49082, acc 0.765625\n",
            "2019-10-29T20:01:46.042740: step 727, loss 0.382744, acc 0.859375\n",
            "2019-10-29T20:01:46.676830: step 728, loss 0.49738, acc 0.75\n",
            "2019-10-29T20:01:47.307031: step 729, loss 0.519046, acc 0.765625\n",
            "2019-10-29T20:01:47.939929: step 730, loss 0.523518, acc 0.75\n",
            "2019-10-29T20:01:48.564199: step 731, loss 0.434141, acc 0.796875\n",
            "2019-10-29T20:01:49.194716: step 732, loss 0.369569, acc 0.859375\n",
            "2019-10-29T20:01:49.822001: step 733, loss 0.529918, acc 0.75\n",
            "2019-10-29T20:01:50.449702: step 734, loss 0.414817, acc 0.78125\n",
            "2019-10-29T20:01:51.071350: step 735, loss 0.455072, acc 0.796875\n",
            "2019-10-29T20:01:51.700165: step 736, loss 0.43244, acc 0.78125\n",
            "2019-10-29T20:01:52.323252: step 737, loss 0.356374, acc 0.828125\n",
            "2019-10-29T20:01:52.947392: step 738, loss 0.453293, acc 0.8125\n",
            "2019-10-29T20:01:53.583822: step 739, loss 0.529913, acc 0.71875\n",
            "2019-10-29T20:01:54.202222: step 740, loss 0.367788, acc 0.84375\n",
            "2019-10-29T20:01:54.829163: step 741, loss 0.434209, acc 0.75\n",
            "2019-10-29T20:01:55.461759: step 742, loss 0.436866, acc 0.8125\n",
            "2019-10-29T20:01:56.098183: step 743, loss 0.363128, acc 0.859375\n",
            "2019-10-29T20:01:56.728878: step 744, loss 0.525274, acc 0.671875\n",
            "2019-10-29T20:01:57.377244: step 745, loss 0.569303, acc 0.703125\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmw7mBxp1j82",
        "colab_type": "text"
      },
      "source": [
        "**The accuracy was the same in the case when the parameters were changed**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSZDRfrhxBc8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}